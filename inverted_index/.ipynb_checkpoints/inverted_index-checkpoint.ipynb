{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from enum import Enum\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares\n",
    "\n",
    "Aqui são criados classes e métodos auxiliares para a tokenização e classificação dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attrs_by_freq(extractions):\n",
    "    attrs = {}\n",
    "    for index, extraction in enumerate(extractions):\n",
    "        for attr in extraction.keys():\n",
    "            if attr not in attrs:\n",
    "                attrs[attr] = []\n",
    "            attrs[attr].append(index)\n",
    "\n",
    "    attr_index = list(attrs.items())\n",
    "\n",
    "    # Sort by frequency (higher to lower)\n",
    "    attr_index.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "    return attr_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização\n",
    "\n",
    "Aqui são definidos duas classes, a primeira é um `Enum` que organiza os possíveis tipos de tokens. A segunda é um tokenizador, capaz de remover _stopwords_, utilizando o nltk, além de filtrar apenas os tipos de tokens desejados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETokenType(Enum):\n",
    "    \"\"\"\n",
    "    Enumerable class with all token's types.\n",
    "    Update this enum every time a new regex group is added to WordTokenizer._token_pattern\n",
    "    The order of the values must match with WordTokenizer._token_pattern regexes' order\n",
    "    \"\"\"\n",
    "    EMAIL = 0\n",
    "    URL = 1\n",
    "    GLUED_TITLES = 2\n",
    "    GLUED_WORD = 3\n",
    "    GLUED_LOWER = 4\n",
    "    TELEPHONE_CEP = 5\n",
    "    VALUE = 6\n",
    "    DATE = 7\n",
    "    GLUED_VALUE = 8\n",
    "    WORD = 9\n",
    "    NON_WORD = 10\n",
    "\n",
    "class WordTokenizer(object):\n",
    "\n",
    "    # _token_pattern holds its state across instances of WordTokenizer\n",
    "    # Every time a new regex group is added to _token_pattern, ETokenType must be updated\n",
    "    # The order of the regexes' order must match with ETokenType values' order\n",
    "    _token_pattern = r\"\"\"(?x)           # Set flag to allow verbose regexps\n",
    "        ([\\w\\.-]+@[\\w\\.-]+(?:\\.[\\w]+)+) # E-mail regex\n",
    "        | (                             # URL regex\n",
    "            (?:http(?:s)?(?::)?(?:\\\\\\\\)?)?  # Optional http or https followed by optional : and //\n",
    "            (?:[a-z0-9_-]+\\.)?              # Optional domain\n",
    "            [a-z0-9_-]+                     # host\n",
    "            (?:\\.\n",
    "                (?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\n",
    "            )+\n",
    "            (?::[0-9]+)?                    # Optional port\n",
    "            (?!\\w)(?:\\/(?:[^\\s\\.,]|[\\.,][^\\s\\.,])+)*(?![^\\.,]$)  # Optional relative URI\n",
    "        )\n",
    "        | ([A-Z][a-z]+(?=\\.?(?:[A-Z][A-Za-z]|\\d)+)) # Capture titles glued to digits or other words\n",
    "        | ([A-Z][A-Za-z]+(?=\\.?(?:[A-Z][a-z]|\\d)+)) # Capture words glued to digits or other words\n",
    "        | ([a-z]+(?=\\.?(?:[A-Z]|\\d)+))              # Capture lower words glued to digits or captalized words\n",
    "        | (         # Capture telephones and CEPs\n",
    "            (?:         # Asserts telephones\n",
    "                (?:(?:\\(?\\ *)\\d{2,3}(?:\\ *\\))?)?    # Gets the DDD\n",
    "                (?:\\ *9\\ *(?:\\.|-|\\/|\\\\)?)?         # Optional ninth digit\n",
    "                (?!(?:1|2)\\d{3})        # Negative lookahead to prevent from getting years\n",
    "                \\d{4}(?:\\.|-|\\/|\\\\)?        # First 4 telephone digits with optional separator\n",
    "                \\d{4}                       # Last 4 digits\n",
    "            ) | (?:     # Asserts CEPs\n",
    "                \\d{2}(?:\\.|-|\\/|\\\\)?    # First two digits, followed by an optional separator\n",
    "                \\d{3}(?:\\.|-|\\/|\\\\)?    # Following three digits, followed by an optional separator\n",
    "                \\d{3}                   # Last three digits\n",
    "            )   # Since the CEPs regex gets some telephones as false positives\n",
    "        )       # both regexes are in same group\n",
    "        | (             # Capture values (as in currencies, percentage, measures...)\n",
    "            (?<![\\d\\.\\/\\\\-])        # Negative lookbehind for digits or separators\n",
    "            (?:(?:R?\\$|€)(?:\\ )*)?  # Currencies symbols\n",
    "            (?!(?:1|2)\\d{3})        # Negative lookahead to prevent from getting years\n",
    "            \\d+                     # Proper digits\n",
    "            (?:\n",
    "                (?:\\.|,)            # Punctuation\n",
    "                (?!(?:1|2)\\d{3})    # Negative lookahead to prevent from getting years\n",
    "                \\d+                 # After punctuation digits\n",
    "            )*\n",
    "            (?:%|\\w{1,3}\\b)?        # Percentage or measures abbreviations\n",
    "            # (?![\\d\\.\\/\\\\-])         # Negative lookahead for digits or separators TODO: Fix it by 15%15%9999999999911111 199999999999999 12-1999 janeiro/2000 09/9/2000\n",
    "        )\n",
    "        | (         # Date regex\n",
    "            # (?<![\\d])   # Negative lookbehind for digits\n",
    "            (?:(?:0?[1-9]|[1-2][0-9]|3[0-1])(?!\\d)(?:\\.|-|\\/|\\\\))?    # Asserts the first of three parts of a date (optional)\n",
    "            (?:(?:[A-Za-z_]+|0?[1-9]|[1-2][0-9]|3[0-1])(?!\\d)(?:\\.|-|\\/|\\\\))?   # Asserts the second part, can be either a word or one to two digits (optional)\n",
    "            (?:(?:(?:1|2)\\d{3})|[0-9]{2})(?!\\d)                       # Asserts the year\n",
    "        )\n",
    "        | (     # Capture (glued) values (as in currencies, percentage, measures...)\n",
    "            (?:(?:R?\\$|€)(?:\\ )*)?  # Currencies symbols\n",
    "            \\d+                     # Proper digits\n",
    "            (?:(?:\\.|,)\\d+)*        # Punctuation\n",
    "            (?:%|\\w{1,3}\\b)?        # Percentage or measures abbreviations\n",
    "        )       # This second search aims to get values that were glued to digits or separators\n",
    "        | ((?:\\w+\\.?)*(?:\\w+))   # Words and abbreviations with optional : at the end\n",
    "        | ([^A-Za-z0-9\\ \\n])    # Every thing that is not a letter, a digit, space or line break\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def token_pattern(self):\n",
    "        \"\"\"\n",
    "        Read-only property. This property holds its state across instances of WordTokenizer.\n",
    "        \"\"\"\n",
    "        return self._token_pattern\n",
    "\n",
    "    @property\n",
    "    def stopwords(self):\n",
    "        # TODO: Set self.remove_stopwords setter to also set self._stopwords considering the lang\n",
    "        #       Also set the lang setter to change self._stopwords accordingly\n",
    "        \"\"\"\n",
    "        Read-only property. Returns the list of stopwords if and only if\n",
    "        self._remove_stopwords is True\n",
    "        \"\"\"\n",
    "        if self.remove_stopwords:\n",
    "            if self._stopwords is None:\n",
    "                self._stopwords = nltk.corpus.stopwords.words(self.lang)\n",
    "            return self._stopwords\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __init__(self, lang, remove_stopwords=False, lower_case=False, do_stemming=False):\n",
    "        self.lang = lang\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lower_case = lower_case\n",
    "        self.do_stemming = do_stemming\n",
    "        self._stopwords = None\n",
    "\n",
    "        if self.remove_stopwords:\n",
    "            self._stopwords = nltk.corpus.stopwords.words(lang)\n",
    "        if do_stemming:\n",
    "            self.stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "    def _extract_text(self, html):\n",
    "        ## Regexes for html pages splitting\n",
    "        #  Remove script tags and its content\n",
    "        SCRIPT_TAG_REGEX = re.compile(r'<script.+?>(.|\\n)+?</script>')\n",
    "        STYLE_TAG_REGEX = re.compile(r'<style.+?>(.|\\n)+?</style>')\n",
    "        # Remove remaining tags, leaving content\n",
    "        HTML_TAGS_REGEX = re.compile(r'<[^>]*>')\n",
    "\n",
    "        return HTML_TAGS_REGEX.sub(' ', SCRIPT_TAG_REGEX.sub(' ', html))\n",
    "\n",
    "    def _shave_marks(self, text):\n",
    "        \"\"\"\n",
    "        Removes all diacritic marks from the given string\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return ''\n",
    "\n",
    "        norm_text = unicodedata.normalize('NFD', text)\n",
    "        shaved = ''.join(char for char in norm_text if not unicodedata.combining(char))\n",
    "        return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "    def _tag_tokens(self, document_tokens):\n",
    "        typed_tokens = []\n",
    "        for match_group in document_tokens:\n",
    "            typed_group = []\n",
    "            for index, match in enumerate(match_group):\n",
    "                if match:\n",
    "                    typed_group.append((ETokenType(index), match))\n",
    "\n",
    "            # if typed_group:\n",
    "            assert len(typed_group) > 0, \"Token with no match, probably missing parenthesis on regex\"\n",
    "            assert len(typed_group) == 1, \"Multiple matches for a single token %r\" % ' '.join(match_group)\n",
    "            typed_tokens.append(typed_group[0])\n",
    "\n",
    "        return typed_tokens\n",
    "\n",
    "    def tokenize(self, html, ignored_token_types=[], min_token_size=2):\n",
    "        \"\"\"\n",
    "        Tokenize a string by: e-mail, url, date, glued words, values, abbreviations, words and\n",
    "        every thing that isn't a letter, digit, blank space or line break.\n",
    "\n",
    "        Returning only tokens of desirable types\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract text from html document\n",
    "        text = self._extract_text(html)\n",
    "\n",
    "        # Remove diacritcs\n",
    "        shaved_text = self._shave_marks(text)\n",
    "        \n",
    "        # Returns an array where every position has a tuple with one position to\n",
    "        # every regex on token_pattern\n",
    "        document_tokens = nltk.regexp_tokenize(shaved_text, self._token_pattern)\n",
    "\n",
    "        # Transform the array of tuples into another array of tuples where\n",
    "        # the first position is the token_type and the second is the token itself\n",
    "        document_tokens = self._tag_tokens(document_tokens)\n",
    "\n",
    "        # Filter token types\n",
    "        document_tokens = [token for token_type, token in document_tokens\n",
    "                           if token_type not in ignored_token_types]\n",
    "\n",
    "        if self.remove_stopwords:\n",
    "            # Keeps tokens that has at least one captalized letter (even if is a stopword)\n",
    "            # Since only lower case words test the second condition, there is no need to lower the token\n",
    "            document_tokens = [token for token in document_tokens\n",
    "                               if not token.islower() or not token in self._stopwords]\n",
    "\n",
    "        if self.lower_case:\n",
    "            document_tokens = [token.lower() for token in document_tokens]\n",
    "\n",
    "        document_tokens = [token.strip() for token in document_tokens if len(token.strip()) >= min_token_size]\n",
    "\n",
    "        if self.do_stemming:\n",
    "            document_tokens = [self.stemmer.stem(token) for token in document_tokens]\n",
    "        \n",
    "        return document_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura e análise dos dados\n",
    "\n",
    "Como resultado do projeto anterior, foi gerado um `json` contendo as extrações dos documentos classificados como relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../extractor/extracao.json', 'r') as f:\n",
    "    extractions = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, temos os atributos das extrações ordenados ela sua frequência. A lista começa do atributo mais frequente até o que menos aparece nas extrações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 3892\n",
      "nome: 3892\n",
      "link: 3892\n",
      "diretorio: 3892\n",
      "elenco: 3735\n",
      "roteiro: 3501\n",
      "ano: 3112\n",
      "direcao: 3037\n",
      "duracao: 2175\n",
      "fotografia: 1422\n",
      "producao: 1374\n",
      "distribuidora: 1158\n",
      "genero: 1067\n",
      "estudio: 775\n",
      "diretor: 757\n",
      "trilha sonora: 744\n",
      "montagem: 703\n",
      "montador: 653\n",
      "classificacao: 646\n",
      "design de producao: 580\n",
      "musica: 522\n",
      "figurino: 507\n",
      "direcao de arte: 278\n",
      "titulo original: 43\n",
      "lancamento: 38\n",
      "nacionalidade: 35\n",
      "info: 14\n",
      "uma frase: 12\n",
      "graus de kb: 12\n",
      "elenco (vozes originais): 10\n",
      "arte: 8\n",
      "data de lancamento: 7\n",
      "elenco (vozes): 6\n",
      "arte-final: 5\n",
      "letras: 5\n",
      "paginas: 5\n",
      "cores: 5\n",
      "canal: 4\n",
      "generos: 4\n",
      "capas: 3\n",
      "editoria: 3\n",
      "vozes originais: 3\n",
      "artista: 2\n",
      "pais: 2\n",
      "gravadora: 2\n",
      "estilo: 2\n",
      "editora original: 2\n",
      "datas originais de publicacao: 2\n",
      "autor: 2\n",
      "editora (nos eua): 2\n",
      "data de publicacao: 2\n",
      "contendo: 2\n",
      "efeitos visuais: 1\n",
      "producao e direcao: 1\n",
      "com: 1\n",
      "criado por: 1\n",
      "datas de escrita e publicacao originais: 1\n",
      "publicacao original: 1\n",
      " direcao: 1\n",
      " roteiro: 1\n",
      "capa: 1\n",
      "elenco (vozes no original): 1\n",
      "elenco (versao original): 1\n",
      "elenco (edicao brasileira  dvd): 1\n",
      "editora: 1\n",
      "no brasil: 1\n",
      "edicoes: 1\n",
      "entrevistados: 1\n",
      "editora (no brasil): 1\n",
      "editora no brasil: 1\n",
      "elenco original: 1\n",
      "nacionalidades: 1\n",
      "direcao e roteiro: 1\n",
      "vozes (original): 1\n",
      "vozes (dubladores nacionais): 1\n",
      "site: 1\n",
      "os comentarios a seguir falam sobre acontecimentos narrados no filme vingadores: 1\n",
      "elenco de vozes: 1\n"
     ]
    }
   ],
   "source": [
    "# Print ordered list of attrs\n",
    "for attr, indices in attrs_by_freq(extractions):\n",
    "    print(attr + ':', len(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `json` contendo as extrações, contém 3 atributos obrigatórios que não dizem respeito às informações extraídas: `'id'`, `'link'` e `'diretorio'`. Este último é o caminho para o documento original, que será útil durante a criação do íncide invertido. \n",
    "\n",
    "Como pode-se notar, existem campos de extração com o mesmo significado, porém com grafias diferentes, como é o caso de `'direcao'`, `' direcao'` e `'diretor'`. Existem ainda campos com informações adicionais, tais informações aparecem - na maioria das vezes - entre parênteses. Outros campos representam múltiplas informações ao mesmo tempo, como por exemplo: `'producao e direcao'`.\n",
    "___\n",
    "\n",
    "# Pré-processamento\n",
    "\n",
    "Nesta etapa serão tratados os casos citados na seção anterior. Adicionalmente, durante esta fase, dados na forma de valores reais serão discretizados.\n",
    "\n",
    "Para normalização dos campos de extração, três providências precisam ser tomadas:\n",
    "- Remoção de informações adicionais;\n",
    "- Remoção da pontuação e espaços adicionais;\n",
    "- Lemmatização de termos similares (sinônimos).\n",
    "\n",
    "## Limpeza das *strings*\n",
    "\n",
    "Em seguida, temos a remoção das informações dentro de parênteses e remoção da pontuação e espaços adicionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer and auxiliar variables\n",
    "tokenizer = WordTokenizer('portuguese', remove_stopwords=True, lower_case=True, do_stemming=False)\n",
    "min_token_size = 2\n",
    "ignored_token_types = [\n",
    "    ETokenType.EMAIL,\n",
    "    ETokenType.URL,\n",
    "    ETokenType.TELEPHONE_CEP,\n",
    "    ETokenType.DATE,\n",
    "    ETokenType.NON_WORD\n",
    "]\n",
    "\n",
    "# Create new list to hold processed extractions\n",
    "proc_extractions = []\n",
    "\n",
    "for extraction in extractions:\n",
    "\n",
    "    # Create new dict to hold processed extraction\n",
    "    proc_extraction = {}\n",
    "\n",
    "    for key in extraction.keys():\n",
    "        # Remove information between parentheses. They are always in the end of the string\n",
    "        proc_key = (re.split(\"[\\(\\)]\", key)[0])\n",
    "\n",
    "        # Some keys will have information of multiple extraction fields\n",
    "        # In this case, there will always be an ' e '\n",
    "        splited_keys = proc_key.split(' e ')\n",
    "\n",
    "        for proc_key in splited_keys:\n",
    "            # Remove any additional space\n",
    "            proc_key = proc_key.strip()\n",
    "\n",
    "            # Process the key and make the whole key be considered a single token\n",
    "            proc_key = '_'.join(\n",
    "                tokenizer.tokenize(\n",
    "                    proc_key,\n",
    "                    ignored_token_types=ignored_token_types,\n",
    "                    min_token_size=min_token_size\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Check if proc_extraction already has values for proc_key\n",
    "            if proc_key in proc_extraction:\n",
    "                # Preserve the current value\n",
    "                proc_extraction[proc_key] = [proc_extraction[proc_key]]\n",
    "                # Append new value\n",
    "                proc_extraction[proc_key].append(extraction[key])\n",
    "            else:\n",
    "                # Add first value for proc_key\n",
    "                proc_extraction[proc_key] = extraction[key]\n",
    "\n",
    "    proc_extractions.append(proc_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a limpeza, já é possível perceber que alguns campos de extração foram combinados, porém nada tão significativo a ponto de mudar a ordem dos mais frequentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 3892\n",
      "nome: 3892\n",
      "link: 3892\n",
      "diretorio: 3892\n",
      "elenco: 3753\n",
      "roteiro: 3503\n",
      "ano: 3112\n",
      "direcao: 3040\n",
      "duracao: 2175\n",
      "fotografia: 1422\n",
      "producao: 1375\n",
      "distribuidora: 1158\n",
      "genero: 1067\n",
      "estudio: 775\n",
      "diretor: 757\n",
      "trilha_sonora: 744\n",
      "montagem: 703\n",
      "montador: 653\n",
      "classificacao: 646\n",
      "design_producao: 580\n",
      "musica: 522\n",
      "figurino: 507\n",
      "direcao_arte: 278\n",
      "titulo_original: 43\n",
      "lancamento: 38\n",
      "nacionalidade: 35\n",
      "info: 14\n",
      "frase: 12\n",
      "graus_kb: 12\n",
      "arte: 8\n",
      "data_lancamento: 7\n",
      "arte_final: 5\n",
      "letras: 5\n",
      "paginas: 5\n",
      "cores: 5\n",
      "canal: 4\n",
      "generos: 4\n",
      "capas: 3\n",
      "editoria: 3\n",
      "editora: 3\n",
      "vozes_originais: 3\n",
      "artista: 2\n",
      "pais: 2\n",
      "gravadora: 2\n",
      "estilo: 2\n",
      "editora_original: 2\n",
      "datas_originais_publicacao: 2\n",
      "autor: 2\n",
      "data_publicacao: 2\n",
      "contendo: 2\n",
      "efeitos_visuais: 1\n",
      ": 1\n",
      "criado: 1\n",
      "datas_escrita: 1\n",
      "publicacao_originais: 1\n",
      "publicacao_original: 1\n",
      "capa: 1\n",
      "brasil: 1\n",
      "edicoes: 1\n",
      "entrevistados: 1\n",
      "editora_brasil: 1\n",
      "elenco_original: 1\n",
      "nacionalidades: 1\n",
      "vozes: 1\n",
      "site: 1\n",
      "comentarios_seguir_falam_sobre_acontecimentos_narrados_filme_vingadores: 1\n",
      "elenco_vozes: 1\n"
     ]
    }
   ],
   "source": [
    "# Print ordered list of attrs\n",
    "for attr, indices in attrs_by_freq(proc_extractions):\n",
    "    print(attr + ':', len(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematização dos campos\n",
    "\n",
    "Agora, os campos com significados parecidos serão combinados. Isso é feito para evitar a descentralização de informações no índice invertido.\n",
    "\n",
    "A seguir, temos um dicionário onde a chave é o `lemma` e o valor é uma lista com todos os sinônimos presentes no corpus. A partir dele, um pequeno índice invertido é criado para ajudar na normalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {\n",
    "    'direcao': ['diretor'],\n",
    "    'montagem': ['montador'],\n",
    "    'trilha_sonora': ['musica'],\n",
    "    'titulo': ['titulo_original', 'nome'],\n",
    "    'lancamento': ['data_lancamento', 'ano'],\n",
    "    'generos': ['genero'],\n",
    "    'arte': ['arte_final'],\n",
    "    'elenco': ['editora_brasil', 'editora_original', 'vozes', 'elenco_vozes', 'vozes_originais'],\n",
    "    'producao': ['design_producao'],\n",
    "    'nacionalidade': ['pais', 'nacionalidades']\n",
    "}\n",
    "\n",
    "synonyms_index = {}\n",
    "for lemma, synonyms in synonyms.items():\n",
    "    synonyms_index[lemma] = lemma\n",
    "    for synonym in synonyms:\n",
    "        synonyms_index[synonym] = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new list to hold lemmatized extractions\n",
    "lem_extractions = []\n",
    "\n",
    "for proc_extraction in proc_extractions:\n",
    "    # Create new dict to hold lemmatized extraction\n",
    "    lem_extraction = {}\n",
    "\n",
    "    for proc_key in proc_extraction:\n",
    "        lem_key = synonyms_index.get(proc_key, proc_key)\n",
    "        \n",
    "        # Check if lem_extraction already has values for lem_key\n",
    "        if lem_key in lem_extraction:\n",
    "            # Preserve the current value\n",
    "            lem_extraction[lem_key] = [lem_extraction[lem_key]]\n",
    "            # Append new value\n",
    "            lem_extraction[lem_key].append(proc_extraction[proc_key])\n",
    "        else:\n",
    "            # Add first value for lem_key\n",
    "            lem_extraction[lem_key] = proc_extraction[proc_key]\n",
    "\n",
    "    lem_extractions.append(lem_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 3892\n",
      "titulo: 3892\n",
      "link: 3892\n",
      "diretorio: 3892\n",
      "direcao: 3797\n",
      "elenco: 3760\n",
      "roteiro: 3503\n",
      "lancamento: 3157\n",
      "duracao: 2175\n",
      "producao: 1484\n",
      "fotografia: 1422\n",
      "montagem: 1356\n",
      "trilha_sonora: 1266\n",
      "distribuidora: 1158\n",
      "generos: 1071\n",
      "estudio: 775\n",
      "classificacao: 646\n",
      "figurino: 507\n",
      "direcao_arte: 278\n",
      "nacionalidade: 38\n",
      "info: 14\n",
      "frase: 12\n",
      "graus_kb: 12\n",
      "arte: 8\n",
      "letras: 5\n",
      "paginas: 5\n",
      "cores: 5\n",
      "canal: 4\n",
      "capas: 3\n",
      "editoria: 3\n",
      "editora: 3\n",
      "artista: 2\n",
      "gravadora: 2\n",
      "estilo: 2\n",
      "datas_originais_publicacao: 2\n",
      "autor: 2\n",
      "data_publicacao: 2\n",
      "contendo: 2\n",
      "efeitos_visuais: 1\n",
      ": 1\n",
      "criado: 1\n",
      "datas_escrita: 1\n",
      "publicacao_originais: 1\n",
      "publicacao_original: 1\n",
      "capa: 1\n",
      "brasil: 1\n",
      "edicoes: 1\n",
      "entrevistados: 1\n",
      "elenco_original: 1\n",
      "site: 1\n",
      "comentarios_seguir_falam_sobre_acontecimentos_narrados_filme_vingadores: 1\n"
     ]
    }
   ],
   "source": [
    "# Print ordered list of attrs\n",
    "for attr, indices in attrs_by_freq(lem_extractions):\n",
    "    print(attr + ':', len(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento dos dados\n",
    "\n",
    "Após as etapas de normalização dos campos de extração, é possível determinar - com certeza - quais os mais frequentes no corpus. São eles:\n",
    "- Título\n",
    "- Direção\n",
    "- Elenco\n",
    "- Roteiro\n",
    "- Lançamento\n",
    "\n",
    "Para os campos de `direcao`, `elenco` e `roteiro`, onde múltiplos nomes podem estar associados ao mesmo `titulo`, os dados serão transformados em listas de tokens para facilitar a criação do índice invertido. Adicionalmente, o campo de `lancamento` será discretizado. Os outros atributos serão tratados como palavras comuns. Sendo assim, receberão o mesmo tratamento de tokenização aplicado para o documento original.\n",
    "\n",
    "Outro ponto que merece atenção é que para diversos filmes, o ano e local de lançamento estão descritos no campo `generos`. O `WordTokenizer` será utilizado para extrair valores numéricos deste atributo para que possam ser adicionados no campo correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer and auxiliar variables\n",
    "tokenizer = WordTokenizer('portuguese', remove_stopwords=True, lower_case=True, do_stemming=True)\n",
    "min_token_size = 2\n",
    "ignore_non_values = [\n",
    "    ETokenType.EMAIL,\n",
    "    ETokenType.URL,\n",
    "    ETokenType.GLUED_TITLES,\n",
    "    ETokenType.GLUED_WORD,\n",
    "    ETokenType.GLUED_LOWER,\n",
    "    ETokenType.TELEPHONE_CEP,\n",
    "    ETokenType.WORD,\n",
    "    ETokenType.NON_WORD\n",
    "]\n",
    "valuable_attr = [\n",
    "    'id', 'link', 'diretorio',\n",
    "    'titulo', 'direcao', 'elenco', 'roteiro', 'lancamento'\n",
    "]\n",
    "\n",
    "# Create new list to hold final extractions\n",
    "clean_extractions = []\n",
    "\n",
    "for lem_extraction in lem_extractions:\n",
    "    # Create new dict to hold final extraction\n",
    "    clean_extraction = {}\n",
    "\n",
    "    # Save all valuables attributes\n",
    "    for attr in valuable_attr:\n",
    "        if attr in lem_extraction:\n",
    "            if attr in clean_extraction:\n",
    "                clean_extraction[attr] = [clean_extraction[attr]]\n",
    "            else:\n",
    "                clean_extraction[attr] = lem_extraction[attr]\n",
    "\n",
    "    # Extract launch date from 'generos'\n",
    "    if 'generos' in lem_extraction:\n",
    "        launch = tokenizer.tokenize(\n",
    "            lem_extraction['generos'],\n",
    "            ignored_token_types=ignore_non_values,\n",
    "            min_token_size=min_token_size\n",
    "        )\n",
    "\n",
    "        if launch:\n",
    "            clean_extraction['lancamento'] = launch\n",
    "\n",
    "    clean_extractions.append(clean_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização das extrações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer and auxiliar variables\n",
    "tokenizer = WordTokenizer('portuguese', remove_stopwords=True, lower_case=True, do_stemming=False)\n",
    "min_token_size = 2\n",
    "ignore_noise = [\n",
    "    ETokenType.EMAIL,\n",
    "    ETokenType.URL,\n",
    "    ETokenType.TELEPHONE_CEP,\n",
    "    ETokenType.NON_WORD\n",
    "]\n",
    "\n",
    "# Attributes that can have multiple values for the same id\n",
    "multivalues = ['direcao', 'elenco', 'roteiro']\n",
    "\n",
    "# Attributes to be ignored by tokenization\n",
    "ignore = ['id', 'link', 'diretorio']\n",
    "\n",
    "# Create new list to hold final extractions\n",
    "final_extractions = []\n",
    "\n",
    "for clean_extraction in clean_extractions:\n",
    "    # Create new dict to hold final extraction\n",
    "    final_extraction = {}\n",
    "\n",
    "    for attr in clean_extraction:\n",
    "        if attr in ignore:\n",
    "            final_extraction[attr] = clean_extraction[attr]\n",
    "            continue\n",
    "\n",
    "        value = clean_extraction[attr]\n",
    "\n",
    "        if type(value) == list:\n",
    "            value = ' '.join(value)\n",
    "\n",
    "        if attr in multivalues:\n",
    "            many = value.split(',')\n",
    "\n",
    "            final_extraction[attr] = []\n",
    "            for one in many:\n",
    "                final_extraction[attr].extend(\n",
    "                    tokenizer.tokenize(\n",
    "                        one,\n",
    "                        ignored_token_types=ignore_noise,\n",
    "                        min_token_size=min_token_size\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            final_extraction[attr] = tokenizer.tokenize(\n",
    "                value,\n",
    "                ignored_token_types=ignore_noise,\n",
    "                min_token_size=min_token_size\n",
    "            )\n",
    "\n",
    "    final_extractions.append(final_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretização dos dados numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "for final_extraction in final_extractions:\n",
    "    if 'lancamento' in final_extraction:\n",
    "        try:\n",
    "            launch = int(final_extraction['lancamento'][0])\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "        else:\n",
    "            if launch < 1950:\n",
    "                discrete_launch = '_1950'\n",
    "            elif launch < 1960:\n",
    "                discrete_launch = '1950_1959'\n",
    "            elif launch < 1970:\n",
    "                discrete_launch = '1960_1969'\n",
    "            elif launch < 1980:\n",
    "                discrete_launch = '1970_1979'\n",
    "            elif launch < 1990:\n",
    "                discrete_launch = '1980_1989'\n",
    "            elif launch < 2000:\n",
    "                discrete_launch = '1990_1999'\n",
    "            elif launch < 2005:\n",
    "                discrete_launch = '2000_2004'\n",
    "            elif launch < 2010:\n",
    "                discrete_launch = '2005_2009'\n",
    "            elif launch < 2013:\n",
    "                discrete_launch = '2010_2012'\n",
    "            elif launch < 2016:\n",
    "                discrete_launch = '2013_2015'\n",
    "            elif launch < 2017:\n",
    "                discrete_launch = '2016_2017'\n",
    "            else:\n",
    "                discrete_launch = '2017_2018'\n",
    "\n",
    "            final_extraction['lancamento'] = [discrete_launch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 3892\n",
      "link: 3892\n",
      "diretorio: 3892\n",
      "titulo: 3892\n",
      "direcao: 3797\n",
      "elenco: 3760\n",
      "roteiro: 3503\n",
      "lancamento: 3171\n"
     ]
    }
   ],
   "source": [
    "# Print ordered list of attrs\n",
    "for attr, indices in attrs_by_freq(final_extractions):\n",
    "    print(attr + ':', len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'link': 'brasil.elpais.com/brasil/2014/01/30/cultura/1391104485_938514.html',\n",
       " 'diretorio': './crawler/pages/heuristic_2/positive_files/pag1.html',\n",
       " 'titulo': ['pele', 'de', 'venus'],\n",
       " 'direcao': ['roman', 'polanski'],\n",
       " 'elenco': ['mathieu', 'amalric', 'emmanuelle', 'seigner'],\n",
       " 'lancamento': ['2013_2015']}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_extractions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice Invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes that can have multiple values for the same id\n",
    "multivalues = ['direcao', 'elenco', 'roteiro']\n",
    "\n",
    "# Attributes to be ignored by the inverted index\n",
    "ignore = ['id', 'link', 'diretorio']\n",
    "\n",
    "inverted_index = {}\n",
    "for final_extraction in final_extractions:\n",
    "    extraction_id = final_extraction['id']\n",
    "\n",
    "    for attr in final_extraction:\n",
    "        if attr in ignore:\n",
    "            continue\n",
    "\n",
    "        for token in final_extraction[attr]:\n",
    "            attr_token = attr + '.' + token\n",
    "\n",
    "            if attr_token not in inverted_index:\n",
    "                inverted_index[attr_token] = (0, [])\n",
    "\n",
    "            inverted_index[attr_token] = (\n",
    "                inverted_index[attr_token][0] + 1,\n",
    "                inverted_index[attr_token][1]\n",
    "            )\n",
    "\n",
    "            inverted_index[attr_token][1].append(extraction_id)\n",
    "\n",
    "    final_extraction['diretorio'] = '.' + final_extraction['diretorio']\n",
    "    with open(final_extraction['diretorio'], 'r') as f:\n",
    "        content = f.read()\n",
    "        tokens = tokenizer.tokenize(\n",
    "            content,\n",
    "            ignored_token_types=ignore_noise,\n",
    "            min_token_size=min_token_size\n",
    "        )\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = (0, {})\n",
    "\n",
    "            inverted_index[token] = (\n",
    "                inverted_index[token][0] + 1,\n",
    "                inverted_index[token][1]\n",
    "            )\n",
    "\n",
    "            if extraction_id not in inverted_index[token][1]:\n",
    "                inverted_index[token][1][extraction_id] = 0\n",
    "            inverted_index[token][1][extraction_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = open('inverted_index.json', 'w+')\n",
    "file_.write(json.dumps(inverted_index, indent=4))\n",
    "file_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118993"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_index.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
