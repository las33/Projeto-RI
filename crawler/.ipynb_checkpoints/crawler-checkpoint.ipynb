{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler de criticas de filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "import urllib.robotparser as robotparser\n",
    "from urllib.request import urlopen\n",
    "import requests as rq\n",
    "from collections import deque\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from random import randrange\n",
    "import json\n",
    "import ssl\n",
    "import selenium\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [ ('Pocilga','pocilga.com.br/'), ('Cinema_com_Rapadura','cinemacomrapadura.com.br/'),('Cineclick','www.cineclick.com.br/'), \n",
    "('Plano_Critico','www.planocritico.com/'),('Plano_Aberto','www.planoaberto.com.br/'),\n",
    "('Cinemasim','www.cinemasim.com.br/'),('Omelete','www.omelete.com.br/'),('Jornal_da_Paraiba','www.jornaldaparaiba.com.br/'),\n",
    "('Cinema_em_Cena','cinemaemcena.cartacapital.com.br/'), ('Elpais','brasil.elpais.com/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando paginas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função que trata caso de pagina com load infinito (https://www.omelete.com.br/criticas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinity_page(page_link):\n",
    "    driver = selenium.webdriver.Firefox(executable_path='./geckodriver/geckodriver')\n",
    "    driver.get(\"https://\"+page_link)\n",
    "    SCROLL_PAUSE_TIME = 1   \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "       \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            try:\n",
    "                driver.find_element_by_id(\"loadMore\").click()\n",
    "                last_height = new_height\n",
    "                continue\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return driver.page_source "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_links_bfs(page):\n",
    "            \n",
    "    home_page =  page[1]\n",
    "    print(home_page)\n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(\"https://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "    except ssl.CertificateError as e:\n",
    "        rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "        rp.read() \n",
    "    \n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5);'}\n",
    "    #arquivos com backup de paginas visitas e fila de paginas\n",
    "    with open('lista_paginas.json') as f:\n",
    "        lista_paginas = json.load(f)\n",
    "    with open('fila_paginas.json') as f:\n",
    "        fila_paginas_json = json.load(f)\n",
    "    \n",
    "    if len(lista_paginas) == 0:\n",
    "        lista_paginas = [home_page]\n",
    "    if len(fila_paginas_json):\n",
    "        fila_paginas = deque(fila_paginas_json)\n",
    "        fila_aux = deque(fila_paginas_json)        \n",
    "    else:\n",
    "        fila_paginas = deque([home_page])\n",
    "        fila_aux = deque([home_page])   \n",
    "    #quantidade de paginas salvas\n",
    "    cont = 0\n",
    "    index_page = 0\n",
    "    while cont < 1000:\n",
    "        page_link = fila_paginas.popleft()\n",
    "                      \n",
    "        try:\n",
    "            resp = rq.get(\"https://\"+page_link, headers=headers, allow_redirects=False)\n",
    "        except rq.exceptions.SSLError:\n",
    "            resp = rq.get(\"http://\"+page_link, headers=headers, allow_redirects=False)\n",
    "        \n",
    "        http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "        html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "        encoding = html_encoding or http_encoding\n",
    "        soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        links = []\n",
    "        #verifica se a pagina veio vazia\n",
    "        if not (resp.content):\n",
    "            continue\n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')\n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".png\") or href.endswith(\".pdf\") or href.endswith(\".jpeg\"):\n",
    "                continue\n",
    "            if \"#\" in href:\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")            \n",
    "            if not (href.startswith(home_page)):\n",
    "                continue                               \n",
    "            if href not in lista_paginas:                    \n",
    "                if cont < 1000:\n",
    "                    #adiciona na fila se não houver problemas com o robots.txt ou pagina = cinema em cena -não possui robots.txt\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        lista_paginas.append(href)\n",
    "                        fila_paginas.append(href)\n",
    "                        fila_aux.append(href)\n",
    "        x = fila_aux.popleft()\n",
    "        cont = cont+1 \n",
    "        \n",
    "        index_page = index_page + 1\n",
    "        page_name = \"page_\"+str(index_page)+\".html\"\n",
    "        with open(\"pages/bfs/\"+page[0]+\"/\" + page_name, 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "        with open('pages/bfs/'+page[0]+\"/pages.json\") as f:\n",
    "            link_number = json.load(f)\n",
    "        link_number.append((page_link,page_name))                    \n",
    "        with open('pages/bfs/'+page[0]+\"/pages.json\", 'w') as outfile:\n",
    "            json.dump(link_number, outfile)\n",
    "        \n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3))\n",
    "        with open('lista_paginas.json', 'w') as outfile:\n",
    "            json.dump(lista_paginas, outfile)\n",
    "        with open('fila_paginas.json', 'w') as outfile:\n",
    "            json.dump(list(fila_aux), outfile)\n",
    "        with open('cont.json', 'w') as outfile:\n",
    "            json.dump(cont, outfile)\n",
    "    \n",
    "\n",
    "# with open('lista_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('fila_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "    \n",
    "    \n",
    "# get_links_bfs(sites[9])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Heurística 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words = [\"critica\",\"resenha\"]\n",
    "\n",
    "ok_words = [\"cultura\", \"filme\",\"cinema\"] \n",
    "\n",
    "bad_words = [\"noticia\", \"serie\", \"lista\",\"eventos\",\"quadrinhos\",\n",
    "             \"temporada\",\"album\", \"literatura\", \"televisao\", \"musica\", \"podcast\", \"sinopse\"]\n",
    "\n",
    "index_words = [\"/page/\", \"/index/\",\"page=\",\"/a/\"]\n",
    "\n",
    "def get_link_score(href,home_page):\n",
    "    \n",
    "    link = href.replace(home_page, \"\")\n",
    "    for word in bad_words:\n",
    "        if word in link:\n",
    "            return -1\n",
    "    for word in good_words:\n",
    "        if word in link:\n",
    "            for word in index_words:\n",
    "                if word in link:\n",
    "                    return 2\n",
    "            return 3\n",
    "    for word in ok_words:\n",
    "        if word in link:\n",
    "            return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_links_heuristic(page):\n",
    "     \n",
    "    home_page =  page[1]\n",
    "    print(home_page)\n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(\"https://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "    except ssl.CertificateError as e:\n",
    "        rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "        \n",
    "    \n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5);'}\n",
    "    #arquivos com backup de paginas visitas e fila de paginas\n",
    "    with open('lista_paginas.json') as f:\n",
    "        lista_paginas = json.load(f)\n",
    "    with open('fila_paginas.json') as f:\n",
    "        fila_paginas_json = json.load(f)\n",
    "    with open('cont.json') as f:\n",
    "        cont = json.load(f)\n",
    "    \n",
    "    if len(lista_paginas) == 0:\n",
    "        lista_paginas = [home_page]\n",
    "    if len(fila_paginas_json):\n",
    "        page_ranking = fila_paginas_json\n",
    "        fila_aux = fila_paginas_json        \n",
    "    else:\n",
    "        page_ranking = [(home_page,1)]\n",
    "        fila_aux = [(home_page,1)]   \n",
    "    #quantidade de paginas salvas\n",
    "    last_cont = 0\n",
    "    index_page = 0\n",
    "    while cont < 1000:\n",
    "        pagina = page_ranking.pop(0)\n",
    "        page_link = pagina[0]\n",
    "        page_score = pagina[1]\n",
    "        \n",
    "        \n",
    "        if(page_link == \"www.omelete.com.br/criticas\"):\n",
    "            print(\"entrou aqui\")\n",
    "            soup = BeautifulSoup(open(\"pages/criticas-omelete.html\"), \"html.parser\")            \n",
    "        else:\n",
    "            try:\n",
    "                resp = rq.get(\"https://\"+page_link, headers=headers, allow_redirects=False)\n",
    "            except rq.exceptions.SSLError:\n",
    "                resp = rq.get(\"http://\"+page_link, headers=headers, allow_redirects=False)\n",
    "            \n",
    "            http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "            html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "            encoding = html_encoding or http_encoding\n",
    "            soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        \n",
    "        \n",
    "        links = []\n",
    "        #verifica se a pagina veio vazia        \n",
    "        x = fila_aux.pop(0)\n",
    "        if not (resp.content):\n",
    "            continue \n",
    "        \n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')\n",
    "            text = link.string\n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".png\") or href.endswith(\".pdf\") or href.endswith(\".jpeg\"):\n",
    "                continue            \n",
    "            if \"#\" in href:\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")\n",
    "           \n",
    "            if not (href.startswith(home_page)):\n",
    "                continue                               \n",
    "            if href not in lista_paginas:                    \n",
    "                if cont < 1000:\n",
    "                    #adiciona na fila se não houver problemas com o robots.txt ou pagina = cinema em cena -não possui robots.txt\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        score = get_link_score(href,home_page)                        \n",
    "                        page_ranking.append((href,score))\n",
    "                        page_ranking.sort(key=lambda x: x[1], reverse=True)\n",
    "                        #print(page_ranking)\n",
    "                        fila_aux = page_ranking\n",
    "            lista_paginas.append(href)            \n",
    "                        \n",
    "        cont = cont+1 \n",
    "        if (page_score >= 2): \n",
    "            index_page = index_page + 1\n",
    "            page_name = \"page_\"+str(index_page)+\".html\"\n",
    "            with open(\"pages/heuristic/\"+page[0]+\"/\" + page_name, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            with open('pages/heuristic/'+page[0]+\"/pages.json\") as f:\n",
    "                link_number = json.load(f)\n",
    "            link_number.append((page_link,page_name))                    \n",
    "            with open('pages/heuristic/'+page[0]+\"/pages.json\", 'w') as outfile:\n",
    "                json.dump(link_number, outfile)\n",
    "        \n",
    "       \n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3))\n",
    "        with open('lista_paginas.json', 'w') as outfile:\n",
    "            json.dump(lista_paginas, outfile)\n",
    "        with open('fila_paginas.json', 'w') as outfile:\n",
    "            json.dump(fila_aux, outfile)\n",
    "        with open('cont.json', 'w') as outfile:\n",
    "            json.dump(cont, outfile)\n",
    "    \n",
    "\n",
    "# with open('lista_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('fila_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('cont.json', 'w') as outfile:\n",
    "#     json.dump(0, outfile)\n",
    "\n",
    "# get_links_heuristic(sites[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Heuristica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words = [\"critica\",\"resenha\", \"/cultura/cinema/\"] \n",
    "import ssl\n",
    "ok_words = [\"cultura\", \"filme\",\"cinema\",\"cine\"] \n",
    "\n",
    "bad_words = [\"noticia\", \"serie\", \"lista\",\"eventos\",\"quadrinhos\",\"governo\", \"tag/discos/\",\"share=\",\"vida_urbana\",\"colunas/\",\n",
    "             \"politica\",\"actualidad\", \"cultura/agenda\", \"tag/fecha/\",\"economia\",\"internacional\",\"esportes/\",\"tv/\",\n",
    "             \"temporada\",\"album\", \"literatura\", \"televisao\",\"critica_literaria/\",\"libros\",\"premios\",\"resenha-de-livro\",\n",
    "             \"musica\", \"podcast\", \"sinopse\", \"clipe\",\"critica_arte/\", \"autor/\", \"emmy\",\"tag/game_of_thrones\" ] \n",
    "\n",
    "index_words = [\"/page/\", \"/index/\",\"page=\",\"/a/\"]\n",
    "\n",
    "def get_link_score_2(href, text, home_page, pai_score):\n",
    "    regexp = re.compile('[0-9]x[0-9]')\n",
    "    \n",
    "    link = href.replace(home_page, \"\")\n",
    "    if regexp.search(link):\n",
    "        return -1    \n",
    "    for word in bad_words:\n",
    "        if word in link:\n",
    "            return -1\n",
    "    for word in good_words:\n",
    "        if word in link:            \n",
    "            for word in index_words:\n",
    "                if word in link:\n",
    "                    #print(href)\n",
    "                    return 3\n",
    "            return 10\n",
    "    for word in ok_words:\n",
    "        if word in link:\n",
    "            if pai_score == 3:\n",
    "                return 4\n",
    "            return 1\n",
    "    if pai_score == 3:\n",
    "                #print(href)\n",
    "                return 2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_links_heuristic_2(page):\n",
    "     \n",
    "    home_page =  page[1]\n",
    "    \n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "    except ssl.CertificateError as e:\n",
    "        rp.set_url(\"https://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "    print(home_page)    \n",
    "    \n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5);'}\n",
    "    #arquivos com backup de paginas visitas e fila de paginas\n",
    "    with open('lista_paginas.json') as f:\n",
    "        lista_paginas = json.load(f)\n",
    "    with open('fila_paginas.json') as f:\n",
    "        fila_paginas_json = json.load(f)\n",
    "    with open('cont.json') as f:\n",
    "        cont = json.load(f)\n",
    "    \n",
    "    if len(lista_paginas) == 0:\n",
    "        lista_paginas = [home_page]\n",
    "    if len(fila_paginas_json):\n",
    "        page_ranking = fila_paginas_json\n",
    "        fila_aux = fila_paginas_json        \n",
    "    else:\n",
    "        page_ranking = [(home_page,1)]\n",
    "        fila_aux = [(home_page,1)]   \n",
    "    #quantidade de paginas salvas\n",
    "    last_cont = cont\n",
    "    index_page = 0\n",
    "    while cont < 1000:\n",
    "        pagina = page_ranking.pop(0)\n",
    "        page_link = pagina[0] \n",
    "        page_score = pagina[1]\n",
    "        #print(page_link, page_score)\n",
    "        if(page_link == \"www.omelete.com.br/criticas\"):\n",
    "            print(\"entrou aqui\")\n",
    "            soup = BeautifulSoup(open(\"pages/criticas-omelete.html\"), \"html.parser\")            \n",
    "        else:\n",
    "            try:\n",
    "                resp = rq.get(\"https://\"+page_link, headers=headers, allow_redirects=False)\n",
    "            except rq.exceptions.SSLError:\n",
    "                resp = rq.get(\"http://\"+page_link, headers=headers, allow_redirects=False)\n",
    "            \n",
    "            http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "            html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "            encoding = html_encoding or http_encoding\n",
    "            soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        links = []\n",
    "        #verifica se a pagina veio vazia        \n",
    "        x = fila_aux.pop(0)\n",
    "        if not (resp.content):\n",
    "            continue \n",
    "        \n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')\n",
    "            \n",
    "            text = link.string\n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".png\") or href.endswith(\".pdf\") or href.endswith(\".jpeg\"):\n",
    "                continue            \n",
    "            if \"#\" in href:\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")\n",
    "            \n",
    "            if not (href.startswith(home_page)):\n",
    "                continue\n",
    "           \n",
    "            if href not in lista_paginas:                \n",
    "                if cont < 1000:\n",
    "                    #print(href)\n",
    "                    #adiciona na fila se não houver problemas com o robots.txt ou pagina = cinema em cena -não possui robots.txt\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        \n",
    "                        score = get_link_score_2(href,text, home_page, page_score)                        \n",
    "                        page_ranking.append((href,score))\n",
    "                        page_ranking.sort(key=lambda x: x[1], reverse=True)                        \n",
    "                        \n",
    "                        fila_aux.append((href,score))\n",
    "                        fila_aux.sort(key=lambda x: x[1], reverse=True)\n",
    "            lista_paginas.append(href)            \n",
    "            \n",
    "        cont = cont+1 \n",
    "        if (page_score >= 2 and page_score != 3):\n",
    "            last_cont = cont\n",
    "            index_page = index_page + 1\n",
    "            page_name = \"page_\"+str(index_page)+\".html\"\n",
    "            with open(\"pages/heuristic_2/\"+page[0]+\"/\" + page_name, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            with open('pages/heuristic_2/'+page[0]+\"/pages.json\") as f:\n",
    "                link_number = json.load(f)\n",
    "            link_number.append((page_link,page_name))                    \n",
    "            with open('pages/heuristic_2/'+page[0]+\"/pages.json\", 'w') as outfile:\n",
    "                json.dump(link_number, outfile)\n",
    "        \n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3))\n",
    "        with open('lista_paginas.json', 'w') as outfile:\n",
    "            json.dump(lista_paginas, outfile)\n",
    "        with open('fila_paginas.json', 'w') as outfile:\n",
    "            json.dump(fila_aux, outfile)\n",
    "        with open('cont.json', 'w') as outfile:\n",
    "            json.dump(cont, outfile)\n",
    "            \n",
    "        if(cont - last_cont) > 30:\n",
    "            print(\"muito tempo ocioso\")\n",
    "            break\n",
    "        if(len(page_ranking) == 0):\n",
    "            print(\"fila vazia\")\n",
    "            break\n",
    "    \n",
    "\n",
    "# with open('lista_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('fila_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('cont.json', 'w') as outfile:\n",
    "#     json.dump(0, outfile)\n",
    "\n",
    "# get_links_heuristic_2(sites[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pag = sites[9]\n",
    "\n",
    "# with open(\"pages/bfs/\"+pag[0]+\"/\" +\"pages.json\") as f:\n",
    "#     bfs_pages = json.load(f)\n",
    "# with open(\"pages/heuristic/\"+pag[0]+\"/\" +\"pages.json\") as f:\n",
    "#     heuristic_pages = json.load(f)\n",
    "# with open(\"pages/heuristic_2/\"+pag[0]+\"/\" +\"pages.json\") as f:\n",
    "#     heuristic_2_pages = json.load(f)\n",
    "\n",
    "# unique_pages = []\n",
    "\n",
    "# for p in bfs_pages:\n",
    "#     unique_pages.append(p[0])\n",
    "# for p in heuristic_pages:\n",
    "#     if p[0] not in unique_pages:\n",
    "#         unique_pages.append(p[0])\n",
    "# for p in heuristic_2_pages:\n",
    "#     if p[0] not in unique_pages:\n",
    "#         unique_pages.append(p[0])\n",
    "# len(unique_pages)\n",
    "# #unique_pages\n",
    "# with open(\"pages/unique_\"+pag[0]+\".json\", 'w') as outfile:\n",
    "#     json.dump(unique_pages, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "from enum import Enum\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "class ETokenType(Enum):\n",
    "    \"\"\"\n",
    "    Enumerable class with all token's types.\n",
    "    Update this enum every time a new regex group is added to WordTokenizer._token_pattern\n",
    "    The order of the values must match with WordTokenizer._token_pattern regexes' order\n",
    "    \"\"\"\n",
    "    EMAIL = 0\n",
    "    URL = 1\n",
    "    GLUED_TITLES = 2\n",
    "    GLUED_WORD = 3\n",
    "    GLUED_LOWER = 4\n",
    "    TELEPHONE_CEP = 5\n",
    "    VALUE = 6\n",
    "    DATE = 7\n",
    "    GLUED_VALUE = 8\n",
    "    WORD = 9\n",
    "    NON_WORD = 10\n",
    "\n",
    "def read_corpus(corpus_dir, lang, ignored_token_types=[], min_token_size=2):\n",
    "    \"\"\"\n",
    "    Read html files from the received directory.\n",
    "\n",
    "    :param corpus_dir: corpus directory\n",
    "    :return: {doc_name:[doc_terms]}\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = WordTokenizer(lang, remove_stopwords=True, lower_case=True, do_stemming=True)\n",
    "\n",
    "    corpus = {}\n",
    "    classes = []\n",
    "    for path, subdirs, files in os.walk(corpus_dir):\n",
    "        if subdirs:\n",
    "            classes = subdirs\n",
    "\n",
    "        cur_class = ''\n",
    "        for class_ in classes:\n",
    "            if class_ in path:\n",
    "                cur_class = class_\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                html = open(path + '/' + file, mode='r', encoding='utf-8').read()\n",
    "            except UnicodeDecodeError:\n",
    "                print('Error reading file:', file)\n",
    "                continue\n",
    "\n",
    "            corpus[cur_class + '_' + file] = tokenizer.tokenize(\n",
    "                html,\n",
    "                ignored_token_types=ignored_token_types,\n",
    "                min_token_size=min_token_size\n",
    "            )\n",
    "\n",
    "    print('Corpus loaded, document count:', len(corpus))\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "class WordTokenizer(object):\n",
    "\n",
    "    # _token_pattern holds its state across instances of WordTokenizer\n",
    "    # Every time a new regex group is added to _token_pattern, ETokenType must be updated\n",
    "    # The order of the regexes' order must match with ETokenType values' order\n",
    "    _token_pattern = r\"\"\"(?x)           # Set flag to allow verbose regexps\n",
    "        ([\\w\\.-]+@[\\w\\.-]+(?:\\.[\\w]+)+) # E-mail regex\n",
    "        | (                             # URL regex\n",
    "            (?:http(?:s)?(?::)?(?:\\\\\\\\)?)?  # Optional http or https followed by optional : and //\n",
    "            (?:[a-z0-9_-]+\\.)?              # Optional domain\n",
    "            [a-z0-9_-]+                     # host\n",
    "            (?:\\.\n",
    "                (?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\n",
    "            )+\n",
    "            (?::[0-9]+)?                    # Optional port\n",
    "            (?!\\w)(?:\\/(?:[^\\s\\.,]|[\\.,][^\\s\\.,])+)*(?![^\\.,]$)  # Optional relative URI\n",
    "        )\n",
    "        | ([A-Z][a-z]+(?=\\.?(?:[A-Z][A-Za-z]|\\d)+)) # Capture titles glued to digits or other words\n",
    "        | ([A-Z][A-Za-z]+(?=\\.?(?:[A-Z][a-z]|\\d)+)) # Capture words glued to digits or other words\n",
    "        | ([a-z]+(?=\\.?(?:[A-Z]|\\d)+))              # Capture lower words glued to digits or captalized words\n",
    "        | (         # Capture telephones and CEPs\n",
    "            (?:         # Asserts telephones\n",
    "                (?:(?:\\(?\\ *)\\d{2,3}(?:\\ *\\))?)?    # Gets the DDD\n",
    "                (?:\\ *9\\ *(?:\\.|-|\\/|\\\\)?)?         # Optional ninth digit\n",
    "                (?!(?:1|2)\\d{3})        # Negative lookahead to prevent from getting years\n",
    "                \\d{4}(?:\\.|-|\\/|\\\\)?        # First 4 telephone digits with optional separator\n",
    "                \\d{4}                       # Last 4 digits\n",
    "            ) | (?:     # Asserts CEPs\n",
    "                \\d{2}(?:\\.|-|\\/|\\\\)?    # First two digits, followed by an optional separator\n",
    "                \\d{3}(?:\\.|-|\\/|\\\\)?    # Following three digits, followed by an optional separator\n",
    "                \\d{3}                   # Last three digits\n",
    "            )   # Since the CEPs regex gets some telephones as false positives\n",
    "        )       # both regexes are in same group\n",
    "        | (             # Capture values (as in currencies, percentage, measures...)\n",
    "            (?<![\\d\\.\\/\\\\-])        # Negative lookbehind for digits or separators\n",
    "            (?:(?:R?\\$|€)(?:\\ )*)?  # Currencies symbols\n",
    "            (?!(?:1|2)\\d{3})        # Negative lookahead to prevent from getting years\n",
    "            \\d+                     # Proper digits\n",
    "            (?:\n",
    "                (?:\\.|,)            # Punctuation\n",
    "                (?!(?:1|2)\\d{3})    # Negative lookahead to prevent from getting years\n",
    "                \\d+                 # After punctuation digits\n",
    "            )*\n",
    "            (?:%|\\w{1,3}\\b)?        # Percentage or measures abbreviations\n",
    "            # (?![\\d\\.\\/\\\\-])         # Negative lookahead for digits or separators TODO: Fix it by 15%15%9999999999911111 199999999999999 12-1999 janeiro/2000 09/9/2000\n",
    "        )\n",
    "        | (         # Date regex\n",
    "            # (?<![\\d])   # Negative lookbehind for digits\n",
    "            (?:(?:0?[1-9]|[1-2][0-9]|3[0-1])(?!\\d)(?:\\.|-|\\/|\\\\))?    # Asserts the first of three parts of a date (optional)\n",
    "            (?:(?:[A-Za-z_]+|0?[1-9]|[1-2][0-9]|3[0-1])(?!\\d)(?:\\.|-|\\/|\\\\))?   # Asserts the second part, can be either a word or one to two digits (optional)\n",
    "            (?:(?:(?:1|2)\\d{3})|[0-9]{2})(?!\\d)                       # Asserts the year\n",
    "        )\n",
    "        | (     # Capture (glued) values (as in currencies, percentage, measures...)\n",
    "            (?:(?:R?\\$|€)(?:\\ )*)?  # Currencies symbols\n",
    "            \\d+                     # Proper digits\n",
    "            (?:(?:\\.|,)\\d+)*        # Punctuation\n",
    "            (?:%|\\w{1,3}\\b)?        # Percentage or measures abbreviations\n",
    "        )       # This second search aims to get values that were glued to digits or separators\n",
    "        | ((?:\\w+\\.?)*(?:\\w+))   # Words and abbreviations with optional : at the end\n",
    "        | ([^A-Za-z0-9\\ \\n])    # Every thing that is not a letter, a digit, space or line break\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def token_pattern(self):\n",
    "        \"\"\"\n",
    "        Read-only property. This property holds its state across instances of WordTokenizer.\n",
    "        \"\"\"\n",
    "        return self._token_pattern\n",
    "\n",
    "    @property\n",
    "    def stopwords(self):\n",
    "        # TODO: Set self.remove_stopwords setter to also set self._stopwords considering the lang\n",
    "        #       Also set the lang setter to change self._stopwords accordingly\n",
    "        \"\"\"\n",
    "        Read-only property. Returns the list of stopwords if and only if\n",
    "        self._remove_stopwords is True\n",
    "        \"\"\"\n",
    "        if self.remove_stopwords:\n",
    "            if self._stopwords is None:\n",
    "                self._stopwords = nltk.corpus.stopwords.words(self.lang)\n",
    "            return self._stopwords\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __init__(self, lang, remove_stopwords=False, lower_case=False, do_stemming=False):\n",
    "        self.lang = lang\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lower_case = lower_case\n",
    "        self.do_stemming = do_stemming\n",
    "        self._stopwords = None\n",
    "\n",
    "        if self.remove_stopwords:\n",
    "            self._stopwords = nltk.corpus.stopwords.words(lang)\n",
    "        if do_stemming:\n",
    "            self.stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "    def _extract_text(self, html):\n",
    "        ## Regexes for html pages splitting\n",
    "        #  Remove script tags and its content\n",
    "        SCRIPT_TAG_REGEX = re.compile(r'<script.+?>(.|\\n)+?</script>')\n",
    "        STYLE_TAG_REGEX = re.compile(r'<style.+?>(.|\\n)+?</style>')\n",
    "        # Remove remaining tags, leaving content\n",
    "        HTML_TAGS_REGEX = re.compile(r'<[^>]*>')\n",
    "\n",
    "        return HTML_TAGS_REGEX.sub(' ', SCRIPT_TAG_REGEX.sub(' ', html))\n",
    "\n",
    "    def _shave_marks(self, text):\n",
    "        \"\"\"\n",
    "        Removes all diacritic marks from the given string\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return ''\n",
    "\n",
    "        norm_text = unicodedata.normalize('NFD', text)\n",
    "        shaved = ''.join(char for char in norm_text if not unicodedata.combining(char))\n",
    "        return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "    def _tag_tokens(self, document_tokens):\n",
    "        typed_tokens = []\n",
    "        for match_group in document_tokens:\n",
    "            typed_group = []\n",
    "            for index, match in enumerate(match_group):\n",
    "                if match:\n",
    "                    typed_group.append((ETokenType(index), match))\n",
    "\n",
    "            # if typed_group:\n",
    "            assert len(typed_group) > 0, \"Token with no match, probably missing parenthesis on regex\"\n",
    "            assert len(typed_group) == 1, \"Multiple matches for a single token %r\" % ' '.join(match_group)\n",
    "            typed_tokens.append(typed_group[0])\n",
    "\n",
    "        return typed_tokens\n",
    "\n",
    "    def tokenize(self, html, ignored_token_types=[], min_token_size=2):\n",
    "        \"\"\"\n",
    "        Tokenize a string by: e-mail, url, date, glued words, values, abbreviations, words and\n",
    "        every thing that isn't a letter, digit, blank space or line break.\n",
    "\n",
    "        Returning only tokens of desirable types\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract text from html document\n",
    "        text = self._extract_text(html)\n",
    "\n",
    "        # Remove diacritcs\n",
    "        shaved_text = self._shave_marks(text)\n",
    "        \n",
    "        # Returns an array where every position has a tuple with one position to\n",
    "        # every regex on token_pattern\n",
    "        document_tokens = nltk.regexp_tokenize(shaved_text, self._token_pattern)\n",
    "\n",
    "        # Transform the array of tuples into another array of tuples where\n",
    "        # the first position is the token_type and the second is the token itself\n",
    "        document_tokens = self._tag_tokens(document_tokens)\n",
    "\n",
    "        # Filter token types\n",
    "        document_tokens = [token for token_type, token in document_tokens\n",
    "                           if token_type not in ignored_token_types]\n",
    "\n",
    "        if self.remove_stopwords:\n",
    "            # Keeps tokens that has at least one captalized letter (even if is a stopword)\n",
    "            # Since only lower case words test the second condition, there is no need to lower the token\n",
    "            document_tokens = [token for token in document_tokens\n",
    "                               if not token.islower() or not token in self._stopwords]\n",
    "\n",
    "        if self.lower_case:\n",
    "            document_tokens = [token.lower() for token in document_tokens]\n",
    "\n",
    "        document_tokens = [token.strip() for token in document_tokens if len(token.strip()) >= min_token_size]\n",
    "\n",
    "        if self.do_stemming:\n",
    "            document_tokens = [self.stemmer.stem(token) for token in document_tokens]\n",
    "        \n",
    "        return document_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/leonardo/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/leonardo/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/leonardo/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator SelectKBest from version 0.20.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "dt = joblib.load('../classifier/decisiontree.joblib')\n",
    "vectorizer = joblib.load('../classifier/vectorizer.joblib')\n",
    "selector = joblib.load('../classifier/featureselector.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file: page_38.html\n",
      "Error reading file: page_35.html\n",
      "Error reading file: page_1.html\n",
      "Corpus loaded, document count: 204\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(\n",
    "    './pages/heuristic/Pocilga/',\n",
    "    'portuguese',\n",
    "    ignored_token_types=[\n",
    "        ETokenType.EMAIL,\n",
    "        ETokenType.URL,\n",
    "        ETokenType.TELEPHONE_CEP,\n",
    "        ETokenType.DATE,\n",
    "        ETokenType.NON_WORD\n",
    "    ],\n",
    "    min_token_size=2\n",
    ")\n",
    "\n",
    "# Separate ids from documents\n",
    "ids, documents = zip(*[(id_, ' '.join(document)) for id_, document in corpus.items()])\n",
    "\n",
    "labels = []\n",
    "for id_ in ids:\n",
    "    labels.append(id_.split('_')[0])\n",
    "\n",
    "labels = np.array(labels)\n",
    "ids = np.array(ids)\n",
    "documents = np.array(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "lista = []\n",
    "cont = 0\n",
    "for c in documents:\n",
    "    vec = vectorizer.transform([c])\n",
    "    new_vec = selector.transform(vec)\n",
    "    x = dt.predict(new_vec)\n",
    "    if(x[0] == 'positive'):\n",
    "        cont = cont + 1\n",
    "print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
