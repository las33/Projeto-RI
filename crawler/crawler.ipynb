{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler de criticas de filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser as robotparser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginas = [('Jornal_da_Paraiba','www.jornaldaparaiba.com.br/'),('Cineclick','www.cineclick.com.br/'), \n",
    "('Cinema_com_Rapadura','cinemacomrapadura.com.br/'),\n",
    "('Plano_Critico','www.planocritico.com/'),('Plano_Aberto','www.planoaberto.com.br/'),\n",
    "('Cinemasim','www.cinemasim.com.br/'), ('Pocilga','pocilga.com.br/')]\n",
    "\n",
    "#('Omelete','http://www.omelete.com.br/'),('Elpais','https://brasil.elpais.com/', ('Cinema_em_Cena','http://cinemaemcena.cartacapital.com.br/')),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando Robots.txt\n",
    "\n",
    "   ##### Site http://cinemaemcena.cartacapital.com.br/ n√£o possui o arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletando paginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.jornaldaparaiba.com.br/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/leonardo/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "oi\n",
      "www.cineclick.com.br/\n",
      "oi\n",
      "cinemacomrapadura.com.br/\n",
      "www.planocritico.com/\n",
      "oi\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4.dammit import EncodingDetector\n",
    "from collections import deque\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "def get_links(page):\n",
    "    home_page =  page[1]\n",
    "    print(home_page)\n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "    rp.read()\n",
    "    \n",
    "    fila_paginas = deque([home_page])\n",
    "    lista_paginas = [home_page]\n",
    "    \n",
    "    headers = {\n",
    "        'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "        'Accept-Language': 'pt-BR',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "    cont = 0\n",
    "    while fila_paginas:\n",
    "        page_link = fila_paginas.popleft()\n",
    "        #print(page_link)\n",
    "        cont = cont+1 \n",
    "        #print(\"asdajsd\")\n",
    "        resp = rq.get(\"http://\"+page_link,allow_redirects=False, headers=headers)\n",
    "        http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "        html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "        encoding = html_encoding or http_encoding\n",
    "        soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        links = []\n",
    "        if not (resp.content):\n",
    "            print(\"oi\")\n",
    "            continue\n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')           \n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".pnj\"):\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")\n",
    "            if not (href.startswith(home_page)):\n",
    "                continue                               \n",
    "            if href not in lista_paginas:                    \n",
    "                if len(lista_paginas) < 1000:\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        lista_paginas.append(href)\n",
    "                        fila_paginas.append(href)\n",
    "        page_link = page_link.replace(\"https://\", \"\")\n",
    "        page_link = page_link.replace(\"http://\", \"\")\n",
    "        page_link = page_link.replace(\"/\", \"_\")                        \n",
    "        with open(\"pages/bfs/\"+page[0]+\"/\"+page_link+\".html\", 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3)+2)\n",
    "        if (cont == 500):\n",
    "            time.sleep(60)\n",
    "    #print(lista_paginas)\n",
    "\n",
    "\n",
    "for p in paginas:\n",
    "    get_links(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
