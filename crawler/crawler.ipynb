{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler de criticas de filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser as robotparser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginas = [ ('Pocilga','pocilga.com.br/'), ('Cinema_com_Rapadura','cinemacomrapadura.com.br/'),('Cineclick','www.cineclick.com.br/'), \n",
    "('Plano_Critico','www.planocritico.com/'),('Plano_Aberto','www.planoaberto.com.br/'),\n",
    "('Cinemasim','www.cinemasim.com.br/'),('Omelete','http://www.omelete.com.br/'),('Jornal_da_Paraiba','www.jornaldaparaiba.com.br/'),\n",
    "('Cinema_em_Cena','cinemaemcena.cartacapital.com.br/'), ('Elpais','brasil.elpais.com/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando paginas\n",
    "\n",
    "### BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4.dammit import EncodingDetector\n",
    "from collections import deque\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from random import randrange\n",
    "import json\n",
    "\n",
    "def get_links_bfs(page):\n",
    "            \n",
    "    home_page =  page[1]\n",
    "    print(home_page)\n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "    rp.read()   \n",
    "    \n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5);'}\n",
    "    #arquivos com backup de paginas visitas e fila de paginas\n",
    "    with open('lista_paginas.json') as f:\n",
    "        lista_paginas = json.load(f)\n",
    "    with open('fila_paginas.json') as f:\n",
    "        fila_paginas_json = json.load(f)\n",
    "    \n",
    "    if len(lista_paginas) == 0:\n",
    "        lista_paginas = [home_page]\n",
    "    if len(fila_paginas_json):\n",
    "        fila_paginas = deque(fila_paginas_json)\n",
    "        fila_aux = deque(fila_paginas_json)        \n",
    "    else:\n",
    "        fila_paginas = deque([home_page])\n",
    "        fila_aux = deque([home_page])   \n",
    "    #quantidade de paginas salvas\n",
    "    cont = 0\n",
    "    while cont < 1000:\n",
    "        page_link = fila_paginas.popleft()\n",
    "              \n",
    "        resp = rq.get(\"http://\"+page_link, headers=headers, allow_redirects=False)\n",
    "        \n",
    "        http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "        html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "        encoding = html_encoding or http_encoding\n",
    "        soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        links = []\n",
    "        #verifica se a pagina veio vazia\n",
    "        if not (resp.content):\n",
    "            continue\n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')\n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".png\") or href.endswith(\".pdf\") or href.endswith(\".jpeg\"):\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")\n",
    "           \n",
    "            if not (href.startswith(home_page)):\n",
    "                continue                               \n",
    "            if href not in lista_paginas:                    \n",
    "                if cont < 1000:\n",
    "                    #adiciona na fila se não houver problemas com o robots.txt ou pagina = cinema em cena -não possui robots.txt\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        lista_paginas.append(href)\n",
    "                        fila_paginas.append(href)\n",
    "                        fila_aux.append(href)\n",
    "        page_link = page_link.replace(\"https://\", \"\")\n",
    "        page_link = page_link.replace(\"http://\", \"\")\n",
    "        page_link = page_link.replace(\"/\", \"_\") \n",
    "        cont = cont+1 \n",
    "        x = fila_aux.popleft()\n",
    "        with open(\"pages/bfs/\"+page[0]+\"/\"+page_link+\".html\", 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3))\n",
    "        with open('lista_paginas.json', 'w') as outfile:\n",
    "            json.dump(lista_paginas, outfile)\n",
    "        with open('fila_paginas.json', 'w') as outfile:\n",
    "            json.dump(list(fila_aux), outfile)\n",
    "    \n",
    "\n",
    "with open('lista_paginas.json', 'w') as outfile:\n",
    "    json.dump([], outfile)\n",
    "with open('fila_paginas.json', 'w') as outfile:\n",
    "    json.dump([], outfile)\n",
    "    \n",
    "    \n",
    "#get_links_bfs(paginas[5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
