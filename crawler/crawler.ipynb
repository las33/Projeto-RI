{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler de criticas de filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "import urllib.robotparser as robotparser\n",
    "from urllib.request import urlopen\n",
    "import requests as rq\n",
    "from collections import deque\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from random import randrange\n",
    "import json\n",
    "import ssl\n",
    "import selenium\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import selenium.webdriverfrom selenium.common.exceptions import NoSuchElementException\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [ ('Pocilga','pocilga.com.br/'), ('Cinema_com_Rapadura','cinemacomrapadura.com.br/'),('Cineclick','www.cineclick.com.br/'), \n",
    "('Plano_Critico','www.planocritico.com/'),('Plano_Aberto','www.planoaberto.com.br/'),\n",
    "('Cinemasim','www.cinemasim.com.br/'),('Omelete','www.omelete.com.br/'),('Jornal_da_Paraiba','www.jornaldaparaiba.com.br/'),\n",
    "('Cinema_em_Cena','cinemaemcena.cartacapital.com.br/'), ('Elpais','brasil.elpais.com/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando paginas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função que trata caso de pagina com load infinito (https://www.omelete.com.br/criticas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinity_page(page_link):\n",
    "    driver = selenium.webdriver.Firefox(executable_path='./geckodriver/geckodriver')\n",
    "    driver.get(\"https://\"+page_link)\n",
    "    SCROLL_PAUSE_TIME = 1   \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "       \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            try:\n",
    "                driver.find_element_by_id(\"loadMore\").click()\n",
    "                last_height = new_height\n",
    "                continue\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return driver.page_source "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_links_bfs(page):\n",
    "            \n",
    "    home_page =  page[1]\n",
    "    print(home_page)\n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(\"https://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "    except ssl.CertificateError as e:\n",
    "        rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "        rp.read() \n",
    "    \n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5);'}\n",
    "    #arquivos com backup de paginas visitas e fila de paginas\n",
    "    with open('lista_paginas.json') as f:\n",
    "        lista_paginas = json.load(f)\n",
    "    with open('fila_paginas.json') as f:\n",
    "        fila_paginas_json = json.load(f)\n",
    "    \n",
    "    if len(lista_paginas) == 0:\n",
    "        lista_paginas = [home_page]\n",
    "    if len(fila_paginas_json):\n",
    "        fila_paginas = deque(fila_paginas_json)\n",
    "        fila_aux = deque(fila_paginas_json)        \n",
    "    else:\n",
    "        fila_paginas = deque([home_page])\n",
    "        fila_aux = deque([home_page])   \n",
    "    #quantidade de paginas salvas\n",
    "    cont = 0\n",
    "    index_page = 0\n",
    "    while cont < 1000:\n",
    "        page_link = fila_paginas.popleft()\n",
    "                      \n",
    "        try:\n",
    "            resp = rq.get(\"https://\"+page_link, headers=headers, allow_redirects=False)\n",
    "        except rq.exceptions.SSLError:\n",
    "            resp = rq.get(\"http://\"+page_link, headers=headers, allow_redirects=False)\n",
    "        \n",
    "        http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "        html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "        encoding = html_encoding or http_encoding\n",
    "        soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        links = []\n",
    "        #verifica se a pagina veio vazia\n",
    "        if not (resp.content):\n",
    "            continue\n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')\n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".png\") or href.endswith(\".pdf\") or href.endswith(\".jpeg\"):\n",
    "                continue\n",
    "            if \"#\" in href:\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")            \n",
    "            if not (href.startswith(home_page)):\n",
    "                continue                               \n",
    "            if href not in lista_paginas:                    \n",
    "                if cont < 1000:\n",
    "                    #adiciona na fila se não houver problemas com o robots.txt ou pagina = cinema em cena -não possui robots.txt\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        lista_paginas.append(href)\n",
    "                        fila_paginas.append(href)\n",
    "                        fila_aux.append(href)\n",
    "        x = fila_aux.popleft()\n",
    "        cont = cont+1 \n",
    "        \n",
    "        index_page = index_page + 1\n",
    "        page_name = \"page_\"+str(index_page)+\".html\"\n",
    "        with open(\"pages/bfs/\"+page[0]+\"/\" + page_name, 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "        with open('pages/bfs/'+page[0]+\"/pages.json\") as f:\n",
    "            link_number = json.load(f)\n",
    "        link_number.append((page_link,page_name))                    \n",
    "        with open('pages/bfs/'+page[0]+\"/pages.json\", 'w') as outfile:\n",
    "            json.dump(link_number, outfile)\n",
    "        \n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3))\n",
    "        with open('lista_paginas.json', 'w') as outfile:\n",
    "            json.dump(lista_paginas, outfile)\n",
    "        with open('fila_paginas.json', 'w') as outfile:\n",
    "            json.dump(list(fila_aux), outfile)\n",
    "        with open('cont.json', 'w') as outfile:\n",
    "            json.dump(cont, outfile)\n",
    "    \n",
    "\n",
    "# with open('lista_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('fila_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "    \n",
    "    \n",
    "# get_links_bfs(sites[9])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Heurística 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words = [\"critica\",\"resenha\"]\n",
    "\n",
    "ok_words = [\"cultura\", \"filme\",\"cinema\"] \n",
    "\n",
    "bad_words = [\"noticia\", \"serie\", \"lista\",\"eventos\",\"quadrinhos\",\n",
    "             \"temporada\",\"album\", \"literatura\", \"televisao\", \"musica\", \"podcast\", \"sinopse\"]\n",
    "\n",
    "index_words = [\"/page/\", \"/index/\",\"page=\",\"/a/\"]\n",
    "\n",
    "def get_link_score(href,home_page):\n",
    "    \n",
    "    link = href.replace(home_page, \"\")\n",
    "    for word in bad_words:\n",
    "        if word in link:\n",
    "            return -1\n",
    "    for word in good_words:\n",
    "        if word in link:\n",
    "            for word in index_words:\n",
    "                if word in link:\n",
    "                    return 2\n",
    "            return 3\n",
    "    for word in ok_words:\n",
    "        if word in link:\n",
    "            return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_links_heuristic(page):\n",
    "     \n",
    "    home_page =  page[1]\n",
    "    print(home_page)\n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(\"https://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "    except ssl.CertificateError as e:\n",
    "        rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "        \n",
    "    \n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5);'}\n",
    "    #arquivos com backup de paginas visitas e fila de paginas\n",
    "    with open('lista_paginas.json') as f:\n",
    "        lista_paginas = json.load(f)\n",
    "    with open('fila_paginas.json') as f:\n",
    "        fila_paginas_json = json.load(f)\n",
    "    with open('cont.json') as f:\n",
    "        cont = json.load(f)\n",
    "    \n",
    "    if len(lista_paginas) == 0:\n",
    "        lista_paginas = [home_page]\n",
    "    if len(fila_paginas_json):\n",
    "        page_ranking = fila_paginas_json\n",
    "        fila_aux = fila_paginas_json        \n",
    "    else:\n",
    "        page_ranking = [(home_page,1)]\n",
    "        fila_aux = [(home_page,1)]   \n",
    "    #quantidade de paginas salvas\n",
    "    last_cont = 0\n",
    "    index_page = 0\n",
    "    while cont < 1000:\n",
    "        pagina = page_ranking.pop(0)\n",
    "        page_link = pagina[0]\n",
    "        page_score = pagina[1]\n",
    "        \n",
    "        \n",
    "        if(page_link == \"www.omelete.com.br/criticas\"):\n",
    "            print(\"entrou aqui\")\n",
    "            soup = BeautifulSoup(open(\"pages/criticas-omelete.html\"), \"html.parser\")            \n",
    "        else:\n",
    "            try:\n",
    "                resp = rq.get(\"https://\"+page_link, headers=headers, allow_redirects=False)\n",
    "            except rq.exceptions.SSLError:\n",
    "                resp = rq.get(\"http://\"+page_link, headers=headers, allow_redirects=False)\n",
    "            \n",
    "            http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "            html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "            encoding = html_encoding or http_encoding\n",
    "            soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        \n",
    "        \n",
    "        links = []\n",
    "        #verifica se a pagina veio vazia        \n",
    "        x = fila_aux.pop(0)\n",
    "        if not (resp.content):\n",
    "            continue \n",
    "        \n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')\n",
    "            text = link.string\n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".png\") or href.endswith(\".pdf\") or href.endswith(\".jpeg\"):\n",
    "                continue            \n",
    "            if \"#\" in href:\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")\n",
    "           \n",
    "            if not (href.startswith(home_page)):\n",
    "                continue                               \n",
    "            if href not in lista_paginas:                    \n",
    "                if cont < 1000:\n",
    "                    #adiciona na fila se não houver problemas com o robots.txt ou pagina = cinema em cena -não possui robots.txt\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        score = get_link_score(href,home_page)                        \n",
    "                        page_ranking.append((href,score))\n",
    "                        page_ranking.sort(key=lambda x: x[1], reverse=True)\n",
    "                        #print(page_ranking)\n",
    "                        fila_aux = page_ranking\n",
    "            lista_paginas.append(href)            \n",
    "                        \n",
    "        cont = cont+1 \n",
    "        if (page_score >= 2): \n",
    "            index_page = index_page + 1\n",
    "            page_name = \"page_\"+str(index_page)+\".html\"\n",
    "            with open(\"pages/heuristic/\"+page[0]+\"/\" + page_name, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            with open('pages/heuristic/'+page[0]+\"/pages.json\") as f:\n",
    "                link_number = json.load(f)\n",
    "            link_number.append((page_link,page_name))                    \n",
    "            with open('pages/heuristic/'+page[0]+\"/pages.json\", 'w') as outfile:\n",
    "                json.dump(link_number, outfile)\n",
    "        \n",
    "       \n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3))\n",
    "        with open('lista_paginas.json', 'w') as outfile:\n",
    "            json.dump(lista_paginas, outfile)\n",
    "        with open('fila_paginas.json', 'w') as outfile:\n",
    "            json.dump(fila_aux, outfile)\n",
    "        with open('cont.json', 'w') as outfile:\n",
    "            json.dump(cont, outfile)\n",
    "    \n",
    "\n",
    "# with open('lista_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('fila_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('cont.json', 'w') as outfile:\n",
    "#     json.dump(0, outfile)\n",
    "\n",
    "# get_links_heuristic(sites[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Heuristica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.omelete.com.br/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/leonardo/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entrou aqui\n"
     ]
    }
   ],
   "source": [
    "good_words = [\"critica\",\"resenha\", \"/cultura/cinema/\"] \n",
    "import ssl\n",
    "ok_words = [\"cultura\", \"filme\",\"cinema\",\"cine\"] \n",
    "\n",
    "bad_words = [\"noticia\", \"serie\", \"lista\",\"eventos\",\"quadrinhos\",\"governo\", \"tag/discos/\",\"share=\",\"vida_urbana\",\"colunas/\",\n",
    "             \"politica\",\"actualidad\", \"cultura/agenda\", \"tag/fecha/\",\"economia\",\"internacional\",\"esportes/\",\"tv/\",\n",
    "             \"temporada\",\"album\", \"literatura\", \"televisao\",\"critica_literaria/\",\"libros\",\"premios\",\"resenha-de-livro\",\n",
    "             \"musica\", \"podcast\", \"sinopse\", \"clipe\",\"critica_arte/\", \"autor/\", \"emmy\",\"tag/game_of_thrones\" ] \n",
    "\n",
    "index_words = [\"/page/\", \"/index/\",\"page=\",\"/a/\"]\n",
    "\n",
    "def get_link_score_2(href, text, home_page, pai_score):\n",
    "    regexp = re.compile('[0-9]x[0-9]')\n",
    "    \n",
    "    link = href.replace(home_page, \"\")\n",
    "    if regexp.search(link):\n",
    "        return -1    \n",
    "    for word in bad_words:\n",
    "        if word in link:\n",
    "            return -1\n",
    "    for word in good_words:\n",
    "        if word in link:            \n",
    "            for word in index_words:\n",
    "                if word in link:\n",
    "                    #print(href)\n",
    "                    return 3\n",
    "            return 10\n",
    "    for word in ok_words:\n",
    "        if word in link:\n",
    "            if pai_score == 3:\n",
    "                return 4\n",
    "            return 1\n",
    "    if pai_score == 3:\n",
    "                #print(href)\n",
    "                return 2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_links_heuristic_2(page):\n",
    "     \n",
    "    home_page =  page[1]\n",
    "    \n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "    except ssl.CertificateError as e:\n",
    "        rp.set_url(\"https://\" +home_page +\"robots.txt\")\n",
    "        rp.read()\n",
    "    print(home_page)    \n",
    "    \n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5);'}\n",
    "    #arquivos com backup de paginas visitas e fila de paginas\n",
    "    with open('lista_paginas.json') as f:\n",
    "        lista_paginas = json.load(f)\n",
    "    with open('fila_paginas.json') as f:\n",
    "        fila_paginas_json = json.load(f)\n",
    "    with open('cont.json') as f:\n",
    "        cont = json.load(f)\n",
    "    \n",
    "    if len(lista_paginas) == 0:\n",
    "        lista_paginas = [home_page]\n",
    "    if len(fila_paginas_json):\n",
    "        page_ranking = fila_paginas_json\n",
    "        fila_aux = fila_paginas_json        \n",
    "    else:\n",
    "        page_ranking = [(home_page,1)]\n",
    "        fila_aux = [(home_page,1)]   \n",
    "    #quantidade de paginas salvas\n",
    "    last_cont = cont\n",
    "    index_page = 0\n",
    "    while cont < 1000:\n",
    "        pagina = page_ranking.pop(0)\n",
    "        page_link = pagina[0] \n",
    "        page_score = pagina[1]\n",
    "        #print(page_link, page_score)\n",
    "        if(page_link == \"www.omelete.com.br/criticas\"):\n",
    "            print(\"entrou aqui\")\n",
    "            soup = BeautifulSoup(open(\"pages/criticas-omelete.html\"), \"html.parser\")            \n",
    "        else:\n",
    "            try:\n",
    "                resp = rq.get(\"https://\"+page_link, headers=headers, allow_redirects=False)\n",
    "            except rq.exceptions.SSLError:\n",
    "                resp = rq.get(\"http://\"+page_link, headers=headers, allow_redirects=False)\n",
    "            \n",
    "            http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "            html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "            encoding = html_encoding or http_encoding\n",
    "            soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        links = []\n",
    "        #verifica se a pagina veio vazia        \n",
    "        x = fila_aux.pop(0)\n",
    "        if not (resp.content):\n",
    "            continue \n",
    "        \n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')\n",
    "            \n",
    "            text = link.string\n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".png\") or href.endswith(\".pdf\") or href.endswith(\".jpeg\"):\n",
    "                continue            \n",
    "            if \"#\" in href:\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")\n",
    "            \n",
    "            if not (href.startswith(home_page)):\n",
    "                continue\n",
    "           \n",
    "            if href not in lista_paginas:                \n",
    "                if cont < 1000:\n",
    "                    #print(href)\n",
    "                    #adiciona na fila se não houver problemas com o robots.txt ou pagina = cinema em cena -não possui robots.txt\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        \n",
    "                        score = get_link_score_2(href,text, home_page, page_score)                        \n",
    "                        page_ranking.append((href,score))\n",
    "                        page_ranking.sort(key=lambda x: x[1], reverse=True)                        \n",
    "                        \n",
    "                        fila_aux.append((href,score))\n",
    "                        fila_aux.sort(key=lambda x: x[1], reverse=True)\n",
    "            lista_paginas.append(href)            \n",
    "            \n",
    "        cont = cont+1 \n",
    "        if (page_score >= 2 and page_score != 3):\n",
    "            last_cont = cont\n",
    "            index_page = index_page + 1\n",
    "            page_name = \"page_\"+str(index_page)+\".html\"\n",
    "            with open(\"pages/heuristic_2/\"+page[0]+\"/\" + page_name, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            with open('pages/heuristic_2/'+page[0]+\"/pages.json\") as f:\n",
    "                link_number = json.load(f)\n",
    "            link_number.append((page_link,page_name))                    \n",
    "            with open('pages/heuristic_2/'+page[0]+\"/pages.json\", 'w') as outfile:\n",
    "                json.dump(link_number, outfile)\n",
    "        \n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3))\n",
    "        with open('lista_paginas.json', 'w') as outfile:\n",
    "            json.dump(lista_paginas, outfile)\n",
    "        with open('fila_paginas.json', 'w') as outfile:\n",
    "            json.dump(fila_aux, outfile)\n",
    "        with open('cont.json', 'w') as outfile:\n",
    "            json.dump(cont, outfile)\n",
    "            \n",
    "        if(cont - last_cont) > 30:\n",
    "            print(\"muito tempo ocioso\")\n",
    "            break\n",
    "        if(len(page_ranking) == 0):\n",
    "            print(\"fila vazia\")\n",
    "            break\n",
    "    \n",
    "\n",
    "# with open('lista_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('fila_paginas.json', 'w') as outfile:\n",
    "#     json.dump([], outfile)\n",
    "# with open('cont.json', 'w') as outfile:\n",
    "#     json.dump(0, outfile)\n",
    "\n",
    "# get_links_heuristic_2(sites[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pag = sites[9]\n",
    "\n",
    "# with open(\"pages/bfs/\"+pag[0]+\"/\" +\"pages.json\") as f:\n",
    "#     bfs_pages = json.load(f)\n",
    "# with open(\"pages/heuristic/\"+pag[0]+\"/\" +\"pages.json\") as f:\n",
    "#     heuristic_pages = json.load(f)\n",
    "# with open(\"pages/heuristic_2/\"+pag[0]+\"/\" +\"pages.json\") as f:\n",
    "#     heuristic_2_pages = json.load(f)\n",
    "\n",
    "# unique_pages = []\n",
    "\n",
    "# for p in bfs_pages:\n",
    "#     unique_pages.append(p[0])\n",
    "# for p in heuristic_pages:\n",
    "#     if p[0] not in unique_pages:\n",
    "#         unique_pages.append(p[0])\n",
    "# for p in heuristic_2_pages:\n",
    "#     if p[0] not in unique_pages:\n",
    "#         unique_pages.append(p[0])\n",
    "# len(unique_pages)\n",
    "# #unique_pages\n",
    "# with open(\"pages/unique_\"+pag[0]+\".json\", 'w') as outfile:\n",
    "#     json.dump(unique_pages, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
