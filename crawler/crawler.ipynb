{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler de criticas de filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser as robotparser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginas = [ ('Pocilga','pocilga.com.br/'),('Cinema_com_Rapadura','cinemacomrapadura.com.br/'),('Cineclick','www.cineclick.com.br/'), \n",
    "('Plano_Critico','www.planocritico.com/'),('Plano_Aberto','www.planoaberto.com.br/'),\n",
    "('Cinemasim','www.cinemasim.com.br/')]\n",
    "\n",
    "#('Omelete','http://www.omelete.com.br/'),('Jornal_da_Paraiba','www.jornaldaparaiba.com.br/'),\n",
    "#('Cinema_em_Cena','cinemaemcena.cartacapital.com.br/')), ('Elpais','brasil.elpais.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando Robots.txt\n",
    "\n",
    "   ##### Site http://cinemaemcena.cartacapital.com.br/ n√£o possui o arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletando paginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pocilga.com.br/\n",
      "leonar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/leonardo/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4.dammit import EncodingDetector\n",
    "from collections import deque\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from random import randrange\n",
    "import json\n",
    "\n",
    "def get_links(page):\n",
    "            \n",
    "    home_page =  page[1]\n",
    "    print(home_page)\n",
    "    ##Carrega o robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(\"http://\" +home_page +\"robots.txt\")\n",
    "    rp.read()   \n",
    "    \n",
    "    #headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"}\n",
    "    \n",
    "    with open('lista_paginas.json') as f:\n",
    "        lista_paginas = json.load(f)\n",
    "    with open('fila_paginas.json') as f:\n",
    "        fila_paginas_json = json.load(f)\n",
    "    \n",
    "    if len(lista_paginas) == 0:\n",
    "        lista_paginas = [home_page]\n",
    "    if len(fila_paginas_json):\n",
    "        print(\"andioahdouahd\")\n",
    "        fila_paginas = deque(fila_paginas_json)\n",
    "        fila_aux = deque(fila_paginas_json)        \n",
    "    else:\n",
    "        print(\"leonar\")\n",
    "        fila_paginas = deque([home_page])\n",
    "        fila_aux = deque([home_page])\n",
    "        \n",
    "    \n",
    "  \n",
    "    cont = 0\n",
    "    while cont < 1000:\n",
    "        #print(fila_paginas )\n",
    "        page_link = fila_paginas.popleft()\n",
    "              \n",
    "        resp = rq.get(\"http://\"+page_link)\n",
    "        \n",
    "        http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "        html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "        encoding = html_encoding or http_encoding\n",
    "        soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "        links = []\n",
    "        if not (resp.content):\n",
    "            continue\n",
    "        pattern = re.compile(\"^(/|http)\")\n",
    "        for link in soup.find_all(\"a\", href=pattern):\n",
    "            href = link.get('href')\n",
    "            \n",
    "            if href.endswith(\".xml\") or href.endswith(\".jpg\") or href.endswith(\".pnj\") or href.endswith(\".pdf\"):\n",
    "                continue\n",
    "            if href.startswith(\"//\"):\n",
    "                href = href[2:] \n",
    "            if href.startswith(\"/\"):\n",
    "                href = home_page + href[1:]                \n",
    "            href = href.replace(\"https://\", \"\")\n",
    "            href = href.replace(\"http://\", \"\")\n",
    "           \n",
    "            if not (href.startswith(home_page)):\n",
    "                continue                               \n",
    "            if href not in lista_paginas:                    \n",
    "                if cont < 1000:\n",
    "                    #print(href)\n",
    "                    if rp.can_fetch(\"*\", href) or (page[0] == 'Cinema_em_Cena'):\n",
    "                        lista_paginas.append(href)\n",
    "                        fila_paginas.append(href)\n",
    "                        fila_aux.append(href)\n",
    "        page_link = page_link.replace(\"https://\", \"\")\n",
    "        page_link = page_link.replace(\"http://\", \"\")\n",
    "        page_link = page_link.replace(\"/\", \"_\") \n",
    "        cont = cont+1 \n",
    "        x = fila_aux.popleft()\n",
    "        with open(\"pages/bfs/\"+page[0]+\"/\"+page_link+\".html\", 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "        if (cont % 10) == 0:\n",
    "            time.sleep(randrange(3))\n",
    "        with open('lista_paginas.json', 'w') as outfile:\n",
    "            json.dump(lista_paginas, outfile)\n",
    "        with open('fila_paginas.json', 'w') as outfile:\n",
    "            json.dump(list(fila_aux), outfile)\n",
    "    #print(lista_paginas)\n",
    "    \n",
    "\n",
    "\n",
    "for p in paginas:\n",
    "    get_links(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
