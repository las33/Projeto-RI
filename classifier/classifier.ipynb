{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares\n",
    "\n",
    "Aqui são criados classes e métodos auxiliares para a tokenização e classificação dos documentos.\n",
    "___\n",
    "\n",
    "## Tokenização\n",
    "\n",
    "- TODO: Adicionar `stemmer` e `lemmatizer`\n",
    "\n",
    "Aqui são definidos duas classes, a primeira é um `Enum` que organiza os possíveis tipos de tokens. A segunda é um tokenizador, capaz de remover _stopwords_, utilizando o nltk, além de filtrar apenas os tipos de tokens desejados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETokenType(Enum):\n",
    "    \"\"\"\n",
    "    Enumerable class with all token's types.\n",
    "    Update this enum every time a new regex group is added to WordTokenizer._token_pattern\n",
    "    The order of the values must match with WordTokenizer._token_pattern regexes' order\n",
    "    \"\"\"\n",
    "    EMAIL = 0\n",
    "    URL = 1\n",
    "    GLUED_TITLES = 2\n",
    "    GLUED_WORD = 3\n",
    "    GLUED_LOWER = 4\n",
    "    TELEPHONE_CEP = 5\n",
    "    VALUE = 6\n",
    "    DATE = 7\n",
    "    GLUED_VALUE = 8\n",
    "    WORD = 9\n",
    "    NON_WORD = 10\n",
    "\n",
    "class WordTokenizer(object):\n",
    "\n",
    "    # _token_pattern holds its state across instances of WordTokenizer\n",
    "    # Every time a new regex group is added to _token_pattern, ETokenType must be updated\n",
    "    # The order of the regexes' order must match with ETokenType values' order\n",
    "    _token_pattern = r\"\"\"(?x)           # Set flag to allow verbose regexps\n",
    "        ([\\w\\.-]+@[\\w\\.-]+(?:\\.[\\w]+)+) # E-mail regex\n",
    "        | (                             # URL regex\n",
    "            (?:http(?:s)?(?::)?(?:\\\\\\\\)?)?  # Optional http or https followed by optional : and //\n",
    "            (?:[a-z0-9_-]+\\.)?              # Optional domain\n",
    "            [a-z0-9_-]+                     # host\n",
    "            (?:\\.\n",
    "                (?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\n",
    "            )+\n",
    "            (?::[0-9]+)?                    # Optional port\n",
    "            (?!\\w)(?:\\/(?:[^\\s\\.,]|[\\.,][^\\s\\.,])+)*(?![^\\.,]$)  # Optional relative URI\n",
    "        )\n",
    "        | ([A-Z][a-z]+(?=\\.?(?:[A-Z][A-Za-z]|\\d)+)) # Capture titles glued to digits or other words\n",
    "        | ([A-Z][A-Za-z]+(?=\\.?(?:[A-Z][a-z]|\\d)+)) # Capture words glued to digits or other words\n",
    "        | ([a-z]+(?=\\.?(?:[A-Z]|\\d)+))              # Capture lower words glued to digits or captalized words\n",
    "        | (         # Capture telephones and CEPs\n",
    "            (?:         # Asserts telephones\n",
    "                (?:(?:\\(?\\ *)\\d{2,3}(?:\\ *\\))?)?    # Gets the DDD\n",
    "                (?:\\ *9\\ *(?:\\.|-|\\/|\\\\)?)?         # Optional ninth digit\n",
    "                (?!(?:1|2)\\d{3})        # Negative lookahead to prevent from getting years\n",
    "                \\d{4}(?:\\.|-|\\/|\\\\)?        # First 4 telephone digits with optional separator\n",
    "                \\d{4}                       # Last 4 digits\n",
    "            ) | (?:     # Asserts CEPs\n",
    "                \\d{2}(?:\\.|-|\\/|\\\\)?    # First two digits, followed by an optional separator\n",
    "                \\d{3}(?:\\.|-|\\/|\\\\)?    # Following three digits, followed by an optional separator\n",
    "                \\d{3}                   # Last three digits\n",
    "            )   # Since the CEPs regex gets some telephones as false positives\n",
    "        )       # both regexes are in same group\n",
    "        | (             # Capture values (as in currencies, percentage, measures...)\n",
    "            (?<![\\d\\.\\/\\\\-])        # Negative lookbehind for digits or separators\n",
    "            (?:(?:R?\\$|€)(?:\\ )*)?  # Currencies symbols\n",
    "            (?!(?:1|2)\\d{3})        # Negative lookahead to prevent from getting years\n",
    "            \\d+                     # Proper digits\n",
    "            (?:\n",
    "                (?:\\.|,)            # Punctuation\n",
    "                (?!(?:1|2)\\d{3})    # Negative lookahead to prevent from getting years\n",
    "                \\d+                 # After punctuation digits\n",
    "            )*\n",
    "            (?:%|\\w{1,3}\\b)?        # Percentage or measures abbreviations\n",
    "            # (?![\\d\\.\\/\\\\-])         # Negative lookahead for digits or separators TODO: Fix it by 15%15%9999999999911111 199999999999999 12-1999 janeiro/2000 09/9/2000\n",
    "        )\n",
    "        | (         # Date regex\n",
    "            # (?<![\\d])   # Negative lookbehind for digits\n",
    "            (?:(?:0?[1-9]|[1-2][0-9]|3[0-1])(?!\\d)(?:\\.|-|\\/|\\\\))?    # Asserts the first of three parts of a date (optional)\n",
    "            (?:(?:[A-Za-z_]+|0?[1-9]|[1-2][0-9]|3[0-1])(?!\\d)(?:\\.|-|\\/|\\\\))?   # Asserts the second part, can be either a word or one to two digits (optional)\n",
    "            (?:(?:(?:1|2)\\d{3})|[0-9]{2})(?!\\d)                       # Asserts the year\n",
    "        )\n",
    "        | (     # Capture (glued) values (as in currencies, percentage, measures...)\n",
    "            (?:(?:R?\\$|€)(?:\\ )*)?  # Currencies symbols\n",
    "            \\d+                     # Proper digits\n",
    "            (?:(?:\\.|,)\\d+)*        # Punctuation\n",
    "            (?:%|\\w{1,3}\\b)?        # Percentage or measures abbreviations\n",
    "        )       # This second search aims to get values that were glued to digits or separators\n",
    "        | ((?:\\w+\\.?)*(?:\\w+))   # Words and abbreviations with optional : at the end\n",
    "        | ([^A-Za-z0-9\\ \\n])    # Every thing that is not a letter, a digit, space or line break\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def token_pattern(self):\n",
    "        \"\"\"\n",
    "        Read-only property. This property holds its state across instances of WordTokenizer.\n",
    "        \"\"\"\n",
    "        return self._token_pattern\n",
    "\n",
    "    @property\n",
    "    def stopwords(self):\n",
    "        # TODO: Set self.remove_stopwords setter to also set self._stopwords considering the lang\n",
    "        #       Also set the lang setter to change self._stopwords accordingly\n",
    "        \"\"\"\n",
    "        Read-only property. Returns the list of stopwords if and only if\n",
    "        self._remove_stopwords is True\n",
    "        \"\"\"\n",
    "        if self.remove_stopwords:\n",
    "            if self._stopwords is None:\n",
    "                self._stopwords = nltk.corpus.stopwords.words(self.lang)\n",
    "            return self._stopwords\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __init__(self, lang, remove_stopwords=False, lower_case=False):\n",
    "        self.lang = lang\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lower_case = lower_case\n",
    "        self._stopwords = None\n",
    "\n",
    "        if self.remove_stopwords:\n",
    "            self._stopwords = nltk.corpus.stopwords.words(lang)\n",
    "\n",
    "    def _extract_text(self, html):\n",
    "        ## Regexes for html pages splitting\n",
    "        #  Remove script tags and its content\n",
    "        SCRIPT_TAG_REGEX = re.compile(r'<script.+?>(.|\\n)+?</script>')\n",
    "        STYLE_TAG_REGEX = re.compile(r'<style.+?>(.|\\n)+?</style>')\n",
    "        # Remove remaining tags, leaving content\n",
    "        HTML_TAGS_REGEX = re.compile(r'<[^>]*>')\n",
    "\n",
    "        return HTML_TAGS_REGEX.sub(' ', SCRIPT_TAG_REGEX.sub(' ', html))\n",
    "\n",
    "    def _shave_marks(self, text):\n",
    "        \"\"\"\n",
    "        Removes all diacritic marks from the given string\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return ''\n",
    "\n",
    "        norm_text = unicodedata.normalize('NFD', text)\n",
    "        shaved = ''.join(char for char in norm_text if not unicodedata.combining(char))\n",
    "        return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "    def _tag_tokens(self, document_tokens):\n",
    "        typed_tokens = []\n",
    "        for match_group in document_tokens:\n",
    "            typed_group = []\n",
    "            for index, match in enumerate(match_group):\n",
    "                if match:\n",
    "                    typed_group.append((ETokenType(index), match))\n",
    "\n",
    "            # if typed_group:\n",
    "            assert len(typed_group) > 0, \"Token with no match, probably missing parenthesis on regex\"\n",
    "            assert len(typed_group) == 1, \"Multiple matches for a single token %r\" % ' '.join(match_group)\n",
    "            typed_tokens.append(typed_group[0])\n",
    "\n",
    "        return typed_tokens\n",
    "\n",
    "    def tokenize(self, html, ignored_token_types=[], min_token_size=2):\n",
    "        \"\"\"\n",
    "        Tokenize a string by: e-mail, url, date, glued words, values, abbreviations, words and\n",
    "        every thing that isn't a letter, digit, blank space or line break.\n",
    "\n",
    "        Returning only tokens of desirable types\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract text from html document\n",
    "        text = self._extract_text(html)\n",
    "\n",
    "        # Remove diacritcs\n",
    "        shaved_text = self._shave_marks(text)\n",
    "        \n",
    "        # Returns an array where every position has a tuple with one position to\n",
    "        # every regex on token_pattern\n",
    "        document_tokens = nltk.regexp_tokenize(shaved_text, self._token_pattern)\n",
    "\n",
    "        # Transform the array of tuples into another array of tuples where\n",
    "        # the first position is the token_type and the second is the token itself\n",
    "        document_tokens = self._tag_tokens(document_tokens)\n",
    "\n",
    "        # Filter token types\n",
    "        document_tokens = [token for token_type, token in document_tokens\n",
    "                           if token_type not in ignored_token_types]\n",
    "\n",
    "        if self.remove_stopwords:\n",
    "            # Keeps tokens that has at least one captalized letter (even if is a stopword)\n",
    "            # Since only lower case words test the second condition, there is no need to lower the token\n",
    "            document_tokens = [token for token in document_tokens\n",
    "                               if not token.islower() or not token in self._stopwords]\n",
    "\n",
    "        if self.lower_case:\n",
    "            document_tokens = [token.lower() for token in document_tokens]\n",
    "\n",
    "        document_tokens = [token.strip() for token in document_tokens if len(token.strip()) >= min_token_size]\n",
    "        \n",
    "        return document_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Aqui temos um método para executar a leitura e tokenização do _corpus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_dir, lang, ignored_token_types=[], min_token_size=2):\n",
    "    \"\"\"\n",
    "    Read html files from the received directory.\n",
    "\n",
    "    :param corpus_dir: corpus directory\n",
    "    :return: {doc_name:[doc_terms]}\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = WordTokenizer(lang, remove_stopwords=True, lower_case=True)\n",
    "\n",
    "    corpus = {}\n",
    "    for _, _, files in os.walk(corpus_dir):\n",
    "        for file in files:\n",
    "            html = open(corpus_dir + '/' + file, mode='r', encoding='utf-8').read()\n",
    "            corpus[file] = tokenizer.tokenize(\n",
    "                html,\n",
    "                ignored_token_types=ignored_token_types,\n",
    "                min_token_size=min_token_size\n",
    "            )\n",
    "\n",
    "    print('Corpus loaded, document count:', len(corpus))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded, document count: 11\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(\n",
    "    './pages',\n",
    "    'portuguese',\n",
    "    ignored_token_types=[\n",
    "        ETokenType.EMAIL,\n",
    "        ETokenType.URL,\n",
    "        ETokenType.TELEPHONE_CEP,\n",
    "        ETokenType.DATE,\n",
    "        ETokenType.NON_WORD\n",
    "    ],\n",
    "    min_token_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostra dos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of document page_9.html: ['critica', 'cargo', 'filme', 'zumbi', 'muita']\n",
      "Preview of document page_7.html: ['supremacia', 'bourne', 'verdadeiro', 'marco', 'filmes']\n",
      "Preview of document page_4.html: ['critica', 'neve', 'negra', 'thriller', 'desinteressante']\n",
      "Preview of document page_2.html: ['critica', 'piratas', 'caribe', 'vinganca', 'salazar']\n",
      "Preview of document page_0.html: ['demonio', 'neon', 'estetica', 'apresenta', 'limites']\n",
      "Preview of document page_6.html: ['filha', 'meu', 'melhor', 'amigo', 'sobre']\n",
      "Preview of document page_5.html: ['critica', 'the', 'titan', 'ficcao', 'cientifica']\n",
      "Preview of document page_10.html: ['critica', 'han', 'solo', 'uma', 'historia']\n",
      "Preview of document page_3.html: ['harry', 'potter', 'enigma', 'principe', 'ousado']\n",
      "Preview of document page_8.html: ['harry', 'potter', 'camara', 'secreta', 'divertido']\n",
      "Preview of document page_1.html: ['kick', 'ass', 'quebrando', 'tudo', 'miscelanea']\n"
     ]
    }
   ],
   "source": [
    "for key in corpus:\n",
    "    print('Preview of document', key + ':', corpus[key][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação dos dados\n",
    "\n",
    "Primeiro separamos os documentos e seus respectivos ids, para utilizarmos os recursos do `scikit-learn` sem complicações, transformamos nossos documentos (lista de termos) em textos corridos (string única)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate ids from documents\n",
    "ids, documents = zip(*[(id_, ' '.join(document)) for id_, document in corpus.items()])\n",
    "\n",
    "ids = list(ids)\n",
    "documents = list(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui criamos a matriz `term_document`, para tal utilizamos a classe `TfidfVectorizer` do pacote `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11x11133 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14896 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 1 and 2-grams features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Documents' order remains the same\n",
    "features = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Preview of features: 11 documents, 11133 features\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "Manually classify pages and create a label list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1] * len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação\n",
    "\n",
    "Aqui são empregados os seguintes algorítmos de classificação:\n",
    "- Naïve\tbayes\n",
    "- Decision tree (J48)\n",
    "- SVM (SMO)\n",
    "- Logistic regression (logistic)\n",
    "- Multilayer perceptron\n",
    "\n",
    "Antes disso, dividiremos os dados entre treinamento e teste\n",
    "\n",
    "## Divisão dos dados\n",
    "\n",
    "Para termos melhor estimativa dos métodos utilizados, aqui definiremos um método para realizar um `K-Fold` nos dados, retornando a média final dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_training(classifier, data, labels, k=5, verbose=0):\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    results = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'fscore': [],\n",
    "        'accuracy': []\n",
    "    }\n",
    "\n",
    "    for train_indexes, test_indexes in kf.split(labels):\n",
    "        train_data, test_data = data[train_indexes], data[test_indexes]\n",
    "        train_labels, test_labels = labels[train_indexes], labels[test_indexes]\n",
    "\n",
    "        # Train and test\n",
    "        classifier.fit(train_data, train_labels)\n",
    "        pred_labels = classifier.predict(test_data)\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(\n",
    "            test_labels, pred_labels, average='weighted'\n",
    "        )\n",
    "        accuracy = accuracy_score(test_labels, pred_labels)\n",
    "\n",
    "        # If verbose, print fold results\n",
    "        if verbose > 1:\n",
    "            print(\n",
    "                classification_report(test_labels, pred_labels)\n",
    "            )\n",
    "            print('Accuracy:', accuracy)\n",
    "\n",
    "        # Save metrics\n",
    "        results['precision'] = precision\n",
    "        results['recall'] = recall\n",
    "        results['fscore'] = fscore\n",
    "        results['accuracy'] = accuracy\n",
    "    \n",
    "    # If verbose, print final results\n",
    "    if verbose > 0:\n",
    "        print(\n",
    "            'Precision mean:', results['precision'].mean(),\n",
    "            'Recall mean:', results['recall'].mean(),\n",
    "            'Fscore mean:', results['fscore'].mean(),\n",
    "            'Accuracy mean:', results['accuracy'].mean(),\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Aqui utilizamos o `MultinomialNB` pois ele aceita uma matriz esparça como input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'fscore': 1.0, 'accuracy': 1.0}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can train with sparse matrix\n",
    "nb = MultinomialNB()\n",
    "\n",
    "kfold_training(nb, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'fscore': 1.0, 'accuracy': 1.0}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "kfold_training(dt, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-f8718ef588af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mkfold_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-be21a40458e4>\u001b[0m in \u001b[0;36mkfold_training\u001b[0;34m(classifier, data, labels, k, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    148\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                          accept_large_sparse=False)\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[0;32m~/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    523\u001b[0m             raise ValueError(\n\u001b[1;32m    524\u001b[0m                 \u001b[0;34m\"The number of classes has to be greater than one; got %d\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                 \" class\" % len(cls))\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(C=0.5, kernel='rbf', degree=8, gamma=0.01, probability=True)\n",
    "\n",
    "kfold_training(svclassifier, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-12fcb628c1bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkfold_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-be21a40458e4>\u001b[0m in \u001b[0;36mkfold_training\u001b[0;34m(classifier, data, labels, k, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1313\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m   1314\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "# solvers: 'liblinear', 'sag', 'saga'\n",
    "lg = LogisticRegression(random_state=0, solver='sag', multi_class='ovr')\n",
    "\n",
    "kfold_training(lg, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'fscore': 1.0, 'accuracy': 1.0}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "\n",
    "kfold_training(mlp, features, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
