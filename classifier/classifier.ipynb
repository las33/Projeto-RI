{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "from enum import Enum\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares\n",
    "\n",
    "Aqui são criados classes e métodos auxiliares para a tokenização e classificação dos documentos.\n",
    "___\n",
    "\n",
    "## Tokenização\n",
    "\n",
    "- TODO: Adicionar `stemmer` e `lemmatizer`\n",
    "\n",
    "Aqui são definidos duas classes, a primeira é um `Enum` que organiza os possíveis tipos de tokens. A segunda é um tokenizador, capaz de remover _stopwords_, utilizando o nltk, além de filtrar apenas os tipos de tokens desejados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETokenType(Enum):\n",
    "    \"\"\"\n",
    "    Enumerable class with all token's types.\n",
    "    Update this enum every time a new regex group is added to WordTokenizer._token_pattern\n",
    "    The order of the values must match with WordTokenizer._token_pattern regexes' order\n",
    "    \"\"\"\n",
    "    EMAIL = 0\n",
    "    URL = 1\n",
    "    GLUED_TITLES = 2\n",
    "    GLUED_WORD = 3\n",
    "    GLUED_LOWER = 4\n",
    "    TELEPHONE_CEP = 5\n",
    "    VALUE = 6\n",
    "    DATE = 7\n",
    "    GLUED_VALUE = 8\n",
    "    WORD = 9\n",
    "    NON_WORD = 10\n",
    "\n",
    "class WordTokenizer(object):\n",
    "\n",
    "    # _token_pattern holds its state across instances of WordTokenizer\n",
    "    # Every time a new regex group is added to _token_pattern, ETokenType must be updated\n",
    "    # The order of the regexes' order must match with ETokenType values' order\n",
    "    _token_pattern = r\"\"\"(?x)           # Set flag to allow verbose regexps\n",
    "        ([\\w\\.-]+@[\\w\\.-]+(?:\\.[\\w]+)+) # E-mail regex\n",
    "        | (                             # URL regex\n",
    "            (?:http(?:s)?(?::)?(?:\\\\\\\\)?)?  # Optional http or https followed by optional : and //\n",
    "            (?:[a-z0-9_-]+\\.)?              # Optional domain\n",
    "            [a-z0-9_-]+                     # host\n",
    "            (?:\\.\n",
    "                (?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\n",
    "            )+\n",
    "            (?::[0-9]+)?                    # Optional port\n",
    "            (?!\\w)(?:\\/(?:[^\\s\\.,]|[\\.,][^\\s\\.,])+)*(?![^\\.,]$)  # Optional relative URI\n",
    "        )\n",
    "        | ([A-Z][a-z]+(?=\\.?(?:[A-Z][A-Za-z]|\\d)+)) # Capture titles glued to digits or other words\n",
    "        | ([A-Z][A-Za-z]+(?=\\.?(?:[A-Z][a-z]|\\d)+)) # Capture words glued to digits or other words\n",
    "        | ([a-z]+(?=\\.?(?:[A-Z]|\\d)+))              # Capture lower words glued to digits or captalized words\n",
    "        | (         # Capture telephones and CEPs\n",
    "            (?:         # Asserts telephones\n",
    "                (?:(?:\\(?\\ *)\\d{2,3}(?:\\ *\\))?)?    # Gets the DDD\n",
    "                (?:\\ *9\\ *(?:\\.|-|\\/|\\\\)?)?         # Optional ninth digit\n",
    "                (?!(?:1|2)\\d{3})        # Negative lookahead to prevent from getting years\n",
    "                \\d{4}(?:\\.|-|\\/|\\\\)?        # First 4 telephone digits with optional separator\n",
    "                \\d{4}                       # Last 4 digits\n",
    "            ) | (?:     # Asserts CEPs\n",
    "                \\d{2}(?:\\.|-|\\/|\\\\)?    # First two digits, followed by an optional separator\n",
    "                \\d{3}(?:\\.|-|\\/|\\\\)?    # Following three digits, followed by an optional separator\n",
    "                \\d{3}                   # Last three digits\n",
    "            )   # Since the CEPs regex gets some telephones as false positives\n",
    "        )       # both regexes are in same group\n",
    "        | (             # Capture values (as in currencies, percentage, measures...)\n",
    "            (?<![\\d\\.\\/\\\\-])        # Negative lookbehind for digits or separators\n",
    "            (?:(?:R?\\$|€)(?:\\ )*)?  # Currencies symbols\n",
    "            (?!(?:1|2)\\d{3})        # Negative lookahead to prevent from getting years\n",
    "            \\d+                     # Proper digits\n",
    "            (?:\n",
    "                (?:\\.|,)            # Punctuation\n",
    "                (?!(?:1|2)\\d{3})    # Negative lookahead to prevent from getting years\n",
    "                \\d+                 # After punctuation digits\n",
    "            )*\n",
    "            (?:%|\\w{1,3}\\b)?        # Percentage or measures abbreviations\n",
    "            # (?![\\d\\.\\/\\\\-])         # Negative lookahead for digits or separators TODO: Fix it by 15%15%9999999999911111 199999999999999 12-1999 janeiro/2000 09/9/2000\n",
    "        )\n",
    "        | (         # Date regex\n",
    "            # (?<![\\d])   # Negative lookbehind for digits\n",
    "            (?:(?:0?[1-9]|[1-2][0-9]|3[0-1])(?!\\d)(?:\\.|-|\\/|\\\\))?    # Asserts the first of three parts of a date (optional)\n",
    "            (?:(?:[A-Za-z_]+|0?[1-9]|[1-2][0-9]|3[0-1])(?!\\d)(?:\\.|-|\\/|\\\\))?   # Asserts the second part, can be either a word or one to two digits (optional)\n",
    "            (?:(?:(?:1|2)\\d{3})|[0-9]{2})(?!\\d)                       # Asserts the year\n",
    "        )\n",
    "        | (     # Capture (glued) values (as in currencies, percentage, measures...)\n",
    "            (?:(?:R?\\$|€)(?:\\ )*)?  # Currencies symbols\n",
    "            \\d+                     # Proper digits\n",
    "            (?:(?:\\.|,)\\d+)*        # Punctuation\n",
    "            (?:%|\\w{1,3}\\b)?        # Percentage or measures abbreviations\n",
    "        )       # This second search aims to get values that were glued to digits or separators\n",
    "        | ((?:\\w+\\.?)*(?:\\w+))   # Words and abbreviations with optional : at the end\n",
    "        | ([^A-Za-z0-9\\ \\n])    # Every thing that is not a letter, a digit, space or line break\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def token_pattern(self):\n",
    "        \"\"\"\n",
    "        Read-only property. This property holds its state across instances of WordTokenizer.\n",
    "        \"\"\"\n",
    "        return self._token_pattern\n",
    "\n",
    "    @property\n",
    "    def stopwords(self):\n",
    "        # TODO: Set self.remove_stopwords setter to also set self._stopwords considering the lang\n",
    "        #       Also set the lang setter to change self._stopwords accordingly\n",
    "        \"\"\"\n",
    "        Read-only property. Returns the list of stopwords if and only if\n",
    "        self._remove_stopwords is True\n",
    "        \"\"\"\n",
    "        if self.remove_stopwords:\n",
    "            if self._stopwords is None:\n",
    "                self._stopwords = nltk.corpus.stopwords.words(self.lang)\n",
    "            return self._stopwords\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __init__(self, lang, remove_stopwords=False, lower_case=False, do_stemming=False):\n",
    "        self.lang = lang\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lower_case = lower_case\n",
    "        self.do_stemming = do_stemming\n",
    "        self._stopwords = None\n",
    "\n",
    "        if self.remove_stopwords:\n",
    "            self._stopwords = nltk.corpus.stopwords.words(lang)\n",
    "        if do_stemming:\n",
    "            self.stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "    def _extract_text(self, html):\n",
    "        ## Regexes for html pages splitting\n",
    "        #  Remove script tags and its content\n",
    "        SCRIPT_TAG_REGEX = re.compile(r'<script.+?>(.|\\n)+?</script>')\n",
    "        STYLE_TAG_REGEX = re.compile(r'<style.+?>(.|\\n)+?</style>')\n",
    "        # Remove remaining tags, leaving content\n",
    "        HTML_TAGS_REGEX = re.compile(r'<[^>]*>')\n",
    "\n",
    "        return HTML_TAGS_REGEX.sub(' ', SCRIPT_TAG_REGEX.sub(' ', html))\n",
    "\n",
    "    def _shave_marks(self, text):\n",
    "        \"\"\"\n",
    "        Removes all diacritic marks from the given string\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return ''\n",
    "\n",
    "        norm_text = unicodedata.normalize('NFD', text)\n",
    "        shaved = ''.join(char for char in norm_text if not unicodedata.combining(char))\n",
    "        return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "    def _tag_tokens(self, document_tokens):\n",
    "        typed_tokens = []\n",
    "        for match_group in document_tokens:\n",
    "            typed_group = []\n",
    "            for index, match in enumerate(match_group):\n",
    "                if match:\n",
    "                    typed_group.append((ETokenType(index), match))\n",
    "\n",
    "            # if typed_group:\n",
    "            assert len(typed_group) > 0, \"Token with no match, probably missing parenthesis on regex\"\n",
    "            assert len(typed_group) == 1, \"Multiple matches for a single token %r\" % ' '.join(match_group)\n",
    "            typed_tokens.append(typed_group[0])\n",
    "\n",
    "        return typed_tokens\n",
    "\n",
    "    def tokenize(self, html, ignored_token_types=[], min_token_size=2):\n",
    "        \"\"\"\n",
    "        Tokenize a string by: e-mail, url, date, glued words, values, abbreviations, words and\n",
    "        every thing that isn't a letter, digit, blank space or line break.\n",
    "\n",
    "        Returning only tokens of desirable types\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract text from html document\n",
    "        text = self._extract_text(html)\n",
    "\n",
    "        # Remove diacritcs\n",
    "        shaved_text = self._shave_marks(text)\n",
    "        \n",
    "        # Returns an array where every position has a tuple with one position to\n",
    "        # every regex on token_pattern\n",
    "        document_tokens = nltk.regexp_tokenize(shaved_text, self._token_pattern)\n",
    "\n",
    "        # Transform the array of tuples into another array of tuples where\n",
    "        # the first position is the token_type and the second is the token itself\n",
    "        document_tokens = self._tag_tokens(document_tokens)\n",
    "\n",
    "        # Filter token types\n",
    "        document_tokens = [token for token_type, token in document_tokens\n",
    "                           if token_type not in ignored_token_types]\n",
    "\n",
    "        if self.remove_stopwords:\n",
    "            # Keeps tokens that has at least one captalized letter (even if is a stopword)\n",
    "            # Since only lower case words test the second condition, there is no need to lower the token\n",
    "            document_tokens = [token for token in document_tokens\n",
    "                               if not token.islower() or not token in self._stopwords]\n",
    "\n",
    "        if self.lower_case:\n",
    "            document_tokens = [token.lower() for token in document_tokens]\n",
    "\n",
    "        document_tokens = [token.strip() for token in document_tokens if len(token.strip()) >= min_token_size]\n",
    "\n",
    "        if self.do_stemming:\n",
    "            document_tokens = [self.stemmer.stem(token) for token in document_tokens]\n",
    "        \n",
    "        return document_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Aqui temos um método para executar a leitura e tokenização do _corpus_.\n",
    "\n",
    "Aplicando o stemmer reduzimos aproximadamente 10000 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_dir, lang, ignored_token_types=[], min_token_size=2):\n",
    "    \"\"\"\n",
    "    Read html files from the received directory.\n",
    "\n",
    "    :param corpus_dir: corpus directory\n",
    "    :return: {doc_name:[doc_terms]}\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = WordTokenizer(lang, remove_stopwords=True, lower_case=True, do_stemming=True)\n",
    "\n",
    "    corpus = {}\n",
    "    classes = []\n",
    "    for path, subdirs, files in os.walk(corpus_dir):\n",
    "        if subdirs:\n",
    "            classes = subdirs\n",
    "\n",
    "        cur_class = ''\n",
    "        for class_ in classes:\n",
    "            if class_ in path:\n",
    "                cur_class = class_\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                html = open(path + '/' + file, mode='r', encoding='utf-8').read()\n",
    "            except UnicodeDecodeError:\n",
    "                print('Error reading file:', file)\n",
    "                continue\n",
    "\n",
    "            corpus[cur_class + '_' + file] = tokenizer.tokenize(\n",
    "                html,\n",
    "                ignored_token_types=ignored_token_types,\n",
    "                min_token_size=min_token_size\n",
    "            )\n",
    "\n",
    "    print('Corpus loaded, document count:', len(corpus))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file: pocilga2.html\n",
      "Corpus loaded, document count: 231\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(\n",
    "    './pages',\n",
    "    'portuguese',\n",
    "    ignored_token_types=[\n",
    "        ETokenType.EMAIL,\n",
    "        ETokenType.URL,\n",
    "        ETokenType.TELEPHONE_CEP,\n",
    "        ETokenType.DATE,\n",
    "        ETokenType.NON_WORD\n",
    "    ],\n",
    "    min_token_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostra dos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of document negative_pocilga11.html: ['the', 'leftov', 'resenh', 'livr', 'pocilg']\n",
      "Preview of document negative_pocilga6.html: ['retrat', 'dorian', 'gray', 'resenh', 'pocilg']\n",
      "Preview of document negative_cinemasim39.html: ['feel', 'pretty', '8211', 'cinem', 'sim']\n",
      "Preview of document negative_pocilga4.html: ['meridi', 'sang', 'resenh', 'livr', 'pocilg']\n",
      "Preview of document negative_cinemaemcasa7.html: ['outr', 'clip', 'mae', 'darren', 'aronofsky']\n",
      "Preview of document negative_rapadura10.html: ['403', 'forbidden', '403', 'forbidden', 'nginx']\n",
      "Preview of document negative_planocritico9.html: ['crit', 'doc', 'who', 'uma', 'questa']\n",
      "Preview of document negative_omelete22.html: ['omelet', 'mai', 'port', 'notic', 'entreten']\n",
      "Preview of document negative_rapadura8.html: ['extermin', 'futur', 'arnold', 'schwarzenegg', 'lind']\n",
      "Preview of document negative_rapadura4.html: ['han', 'sol', 'uma', 'hist', 'st']\n",
      "Preview of document negative_planoaberto0.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_cinemasim17.html: ['crit', 'pai', 'dos', 'dupl', '8211']\n",
      "Preview of document negative_cinemaemcasa34.html: ['crit', 'pabl', 'villac', 'cinem', 'cen']\n",
      "Preview of document negative_cinemaemcasa1.html: ['cinem', 'hong', 'kong', 'destaqu', 'sp']\n",
      "Preview of document negative_cinemasim36.html: ['jalo', 'parais', 'perd', '8211', 'cinem']\n",
      "Preview of document negative_elpais1.html: ['dia', 'orgulh', 'gay', 'os', 'person']\n",
      "Preview of document negative_jornalparaiba0.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_elpais7.html: ['notic', 'sobr', 'fom', 'el', 'pal']\n",
      "Preview of document negative_planoaberto12.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_planocritico1.html: ['crit', 'hom', 'animal', 'anim', 'mund']\n",
      "Preview of document negative_omelete13.html: ['the', 'hummingbird', 'project', 'function', 'var']\n",
      "Preview of document negative_elpais11.html: ['indic', 'frambo', 'our', 'qual', 'sao']\n",
      "Preview of document negative_planocritico64.html: ['crit', 'eu', 'wolverin', '8211', 'plan']\n",
      "Preview of document negative_planocritico48.html: ['crit', 'batman', 'xam', '8211', 'plan']\n",
      "Preview of document negative_pocilga8.html: ['diari', 'ann', 'frank', 'resenh', 'livr']\n",
      "Preview of document negative_jornalparaiba7.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_cinemasim16.html: ['crit', 'invasa', 'londr', '8211', 'cinem']\n",
      "Preview of document negative_omelete7.html: ['omelet', 'mai', 'port', 'notic', 'entreten']\n",
      "Preview of document negative_cineclick5.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_planocritico27.html: ['crit', 'djang', 'livr', 'graphic', 'novel']\n",
      "Preview of document negative_pocilga1.html: ['os', 'tre', 'sarah', 'lotz', 'resenh']\n",
      "Preview of document negative_omelete29.html: ['redirecting', 'redirecting', 'you', 'should', 'be']\n",
      "Preview of document negative_cinemasim30.html: ['8220', 'mein', 'blind', 'dat', 'mit']\n",
      "Preview of document negative_rapadura5.html: ['onc', 'upon', 'tim', 'in', 'hollywood']\n",
      "Preview of document negative_elpais9.html: ['os', 'ganh', 'osc', 'cult', 'el']\n",
      "Preview of document negative_cinemaemcasa6.html: ['ali', 'grac', 'seri', 'mary', 'harron']\n",
      "Preview of document negative_cineclick2.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_jornalparaiba22.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_planoaberto33.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_omelete17.html: ['the', 'wind', 'function', 'var', 'ts']\n",
      "Preview of document negative_planoaberto16.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_jornalparaiba17.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_cinemaemcasa2.html: ['mur', 'estre', 'rj', 'cinem', 'cen']\n",
      "Preview of document negative_elpais28.html: ['fotorreport', 'music', 'sao', 'real', 'bom']\n",
      "Preview of document negative_planoaberto32.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_planoaberto23.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_jornalparaiba15.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_elpais30.html: ['fath', 'john', 'misty', 'amer', 'ano']\n",
      "Preview of document negative_planocritico2.html: ['crit', 'blacksad', 'amarill', '8211', 'plan']\n",
      "Preview of document negative_planocritico11.html: ['crit', 'fio', 'missang', 'mia', 'cout']\n",
      "Preview of document negative_rapadura6.html: ['venom', 'tom', 'hardy', 'michell', 'will']\n",
      "Preview of document negative_omelete9.html: ['redirecting', 'redirecting', 'you', 'should', 'be']\n",
      "Preview of document negative_cinemaemcasa4.html: ['curs', 'decifr', 'padr', 'uma', 'seman']\n",
      "Preview of document negative_cinemasim34.html: ['crit', 'shaolin', 'serta', '8211', 'cinem']\n",
      "Preview of document negative_rapadura1.html: ['we', 'wer', 'nev', 'her', 'livr']\n",
      "Preview of document negative_planoaberto26.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_cineclick1.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_omelete15.html: ['wi', 'fi', 'ralph', 'disney', 'mud']\n",
      "Preview of document negative_cineclick11.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_pocilga7.html: ['ult', 'viag', 'lusitan', 'resenh', 'livr']\n",
      "Preview of document negative_planocritico41.html: ['crit', 'said', 'pel', 'esquerd', 'as']\n",
      "Preview of document negative_rapadura3.html: ['halloween', 'bilhet', 'fim', 'seman', 'estre']\n",
      "Preview of document negative_pocilga10.html: ['arqu', 'crit', 'pagin', 'pocilg', 'img.wp']\n",
      "Preview of document negative_omelete25.html: ['toky', 'ghoul', 'function', 'var', 'ts']\n",
      "Preview of document negative_planocritico55.html: ['crit', 'cas', 'hom', 'aranh', '8211']\n",
      "Preview of document negative_planoaberto1.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_planoaberto5.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_cineclick4.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_planocritico33.html: ['crit', 'ving', 'cost', 'oest', '8211']\n",
      "Preview of document negative_omelete0.html: ['donny', 'brook', 'function', 'var', 'ts']\n",
      "Preview of document negative_rapadura7.html: ['men', 'fenix', 'negr', 'soph', 'turn']\n",
      "Preview of document negative_planoaberto35.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_cinemasim5.html: ['churchill', 'nocrop.w1024.h2147483647', '8211', 'cinem', 'sim']\n",
      "Preview of document negative_planocritico25.html: ['crit', 'tei', 'hom', 'aranh', '8211']\n",
      "Preview of document negative_rapadura2.html: ['the', 'mul', 'film', 'clint', 'eastwood']\n",
      "Preview of document negative_jornalparaiba29.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_cinemasim2.html: ['crit', 'lar', 'crianc', 'peculi', '8211']\n",
      "Preview of document negative_omelete3.html: ['the', 'front', 'runn', 'function', 'var']\n",
      "Preview of document negative_cineclick7.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_elpais3.html: ['u2', 'pass', 'tesour', 'mao', 'band']\n",
      "Preview of document negative_cinemasim14.html: ['be', '952', 'affd', '024', '417']\n",
      "Preview of document negative_pocilga0.html: ['arqu', 'resenh', 'pagin', 'pocilg', 'img.wp']\n",
      "Preview of document negative_jornalparaiba24.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_cinemasim3.html: ['sem', 'titul', '8211', 'cinem', 'sim']\n",
      "Preview of document negative_elpais6.html: ['naipaul', 'mund', 'cult', 'el', 'pa']\n",
      "Preview of document negative_elpais5.html: ['tarsil', 'mom', 'sutil', 'equilibri', 'merec']\n",
      "Preview of document negative_planocritico21.html: ['crit', 'valerian', 'laurelin', 'cidad', 'agu']\n",
      "Preview of document negative_elpais29.html: ['malandr', 'carioc', 'versa', 'espanhol', 'cult']\n",
      "Preview of document negative_elpais4.html: ['notic', 'sobr', 'oliv', 'sack', 'el']\n",
      "Preview of document negative_cineclick9.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document negative_jornalparaiba12.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_omelete2.html: ['vit', 'amp', 'virgin', 'function', 'var']\n",
      "Preview of document negative_pocilga9.html: ['jumanj', 'crit', 'pocilg', 'img.wp', 'smiley']\n",
      "Preview of document negative_elpais21.html: ['resgat', 'est', 'bem', 'est', 'cult']\n",
      "Preview of document negative_pocilga3.html: ['livr', 'st', 'war', 'marc', 'guerr']\n",
      "Preview of document negative_cineclick10.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_elpais17.html: ['notic', 'sobr', 'cinem', 'el', 'pal']\n",
      "Preview of document negative_planoaberto4.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_elpais8.html: ['ant', 'man', 'praz', 'efemer', 'modest']\n",
      "Preview of document negative_jornalparaiba4.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_cineclick8.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_cinemasim10.html: ['la', 'hered', 'film', '8211', 'cinem']\n",
      "Preview of document negative_elpais13.html: ['animal', 'fantas', 'ond', 'habit', 'magizoolog']\n",
      "Preview of document negative_planocritico36.html: ['crit', 'vint', 'mil', 'legu', 'submarin']\n",
      "Preview of document negative_cineclick6.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_cinemaemcasa8.html: ['outr', 'clip', 'mae', 'darren', 'aronofsky']\n",
      "Preview of document negative_jornalparaiba19.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_cinemaemcasa10.html: ['rooney', 'mar', 'ben', 'mendelsohn', 'reviv']\n",
      "Preview of document negative_elpais20.html: ['terri', 'pesadel', 'maravilh', 'seri', 'the']\n",
      "Preview of document negative_cinemaemcasa9.html: ['nov', 'fot', 'it', 'cois', 'adaptaca']\n",
      "Preview of document negative_pocilga5.html: ['novembr', '63', 'resenh', 'livr', 'pocilg']\n",
      "Preview of document negative_omelete16.html: ['boy', 'erased', 'function', 'var', 'ts']\n",
      "Preview of document negative_pocilga12.html: ['put', 'assassin', 'resenh', 'livr', 'pocilg']\n",
      "Preview of document negative_cinemasim12.html: ['tade', 'jon', 'el', 'secret', 'del']\n",
      "Preview of document negative_rapadura0.html: ['notic', 'cinem', 'rapad', 'document', 'ready']\n",
      "Preview of document negative_elpais27.html: ['osc', 'ate', 'ult', 'hom', 'paixa']\n",
      "Preview of document negative_cinemasim38.html: ['crit', 'vi', '8211', 'cinem', 'sim']\n",
      "Preview of document negative_jornalparaiba18.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_cinemaemcasa3.html: ['festiv', 'mim', 'abr', 'inscrico', 'cinem']\n",
      "Preview of document negative_cinemaemcasa0.html: ['notic', 'cinem', 'cen', 'br', 'body']\n",
      "Preview of document negative_planocritico44.html: ['crit', 'turm', 'monic', '8211', 'lac']\n",
      "Preview of document negative_rapadura9.html: ['nov', 'serv', 'streaming', 'appl', 'ter']\n",
      "Preview of document negative_omelete27.html: ['driven', 'function', 'var', 'ts', 'document.createelement']\n",
      "Preview of document negative_elpais24.html: ['glor', 'pires', 'osc', 'estrel', 'dificil']\n",
      "Preview of document negative_planoaberto22.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document negative_cineclick0.html: ['function', 'push', 'gtm.start', 'new', 'dat']\n",
      "Preview of document negative_cinemaemcasa5.html: ['insult', 'indic', 'osc', 'cheg', 'streaming']\n",
      "Preview of document negative_jornalparaiba11.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document negative_omelete11.html: ['teen', 'spirit', 'function', 'var', 'ts']\n",
      "Preview of document negative_jornalparaiba27.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_pocilga11.html: ['crit', 'misteri', 'relogi', 'pared', 'pocilg']\n",
      "Preview of document positive_omelete1.html: ['hotel', 'artemil', 'function', 'var', 'ts']\n",
      "Preview of document positive_pocilga6.html: ['rua', 'cloverfield', '10', 'crit', 'film']\n",
      "Preview of document positive_pocilga4.html: ['crit', 'midnight', 'spec', 'destin', 'espec']\n",
      "Preview of document positive_cinerapadura8.html: ['crit', 'slend', 'man', 'pesadel', 'sem']\n",
      "Preview of document positive_planocritico9.html: ['crit', 'adeu', 'christoph', 'robin', '8211']\n",
      "Preview of document positive_planocritico0.html: ['crit', 'bleach', '8211', 'plan', 'crit']\n",
      "Preview of document positive_planoaberto0.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_elpais1.html: ['ant', 'man', 'praz', 'efemer', 'modest']\n",
      "Preview of document positive_jornalparaiba0.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_omelete8.html: ['misteri', 'relogi', 'pared', 'function', 'var']\n",
      "Preview of document positive_elpais7.html: ['crit', 'pant', 'negr', 'wakand', 'pod']\n",
      "Preview of document positive_cinerapadura7.html: ['crit', 'ond', 'est', 'voc', 'joa']\n",
      "Preview of document positive_planocritico1.html: ['crit', 'funny', 'girl', '8211', 'uma']\n",
      "Preview of document positive_jornalparaiba8.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_jornalparaiba2.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_cinemaemcena4.html: ['as', 'boa', 'man', 'cinem', 'cen']\n",
      "Preview of document positive_pocilga8.html: ['crit', 'menin', 'mund', 'pocilg', 'img.wp']\n",
      "Preview of document positive_jornalparaiba7.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_omelete5.html: ['pred', 'function', 'var', 'ts', 'document.createelement']\n",
      "Preview of document positive_omelete7.html: ['prim', 'noit', 'crim', 'function', 'var']\n",
      "Preview of document positive_cineclick5.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_pocilga1.html: ['crit', 'as', 'boa', 'man', 'pocilg']\n",
      "Preview of document positive_cinemasim4.html: ['cro', 'famil', 'crit', '8211', 'cinem']\n",
      "Preview of document positive_planocritico5.html: ['crit', 'uma', 'questa', 'pessoal', '8211']\n",
      "Preview of document positive_elpais9.html: ['tully', 'charliz', 'theron', 'mulh', 'atriz']\n",
      "Preview of document positive_cineclick2.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_planocritico7.html: ['crit', 'busc', '8230', '8211', 'plan']\n",
      "Preview of document positive_omelete4.html: ['destroy', 'function', 'var', 'ts', 'document.createelement']\n",
      "Preview of document positive_cinemasim7.html: ['par', 'tod', 'os', 'garot', 'que']\n",
      "Preview of document positive_cinemaemcena5.html: ['hereditari', 'cinem', 'cen', 'br', 'public']\n",
      "Preview of document positive_cinerapadura0.html: ['crit', 'paci', 'cas', 'tancred', 'nev']\n",
      "Preview of document positive_cinerapadura9.html: ['crit', 'perfeit', 'pra', 'voc', 'formul']\n",
      "Preview of document positive_jornalparaiba9.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_planocritico2.html: ['crit', 'simples', 'marth', '8211', 'plan']\n",
      "Preview of document positive_planoaberto3.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_omelete9.html: ['american', 'woman', 'function', 'var', 'ts']\n",
      "Preview of document positive_planoaberto7.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_cinemaemcena1.html: ['ilh', 'cachorr', 'cinem', 'cen', 'br']\n",
      "Preview of document positive_planocritico6.html: ['crit', 'nov', 'ond', 'imper', '8211']\n",
      "Preview of document positive_cinemaemcena7.html: ['jurassic', 'world', 'rein', 'ameac', 'cinem']\n",
      "Preview of document positive_cineclick1.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_cinemasim1.html: ['retorn', 'hero', 'crit', '8211', 'cinem']\n",
      "Preview of document positive_pocilga10.html: ['vi', 'ps', 'crit', 'pocilg', 'img.wp']\n",
      "Preview of document positive_planoaberto1.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_planoaberto5.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_cineclick4.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_planocritico8.html: ['crit', 'laranj', 'mecan', '8211', 'plan']\n",
      "Preview of document positive_omelete0.html: ['busc', 'function', 'var', 'ts', 'document.createelement']\n",
      "Preview of document positive_jornalparaiba5.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_cinemaemcena9.html: ['in', 'the', 'gam', 'cinem', 'cen']\n",
      "Preview of document positive_cinemasim5.html: ['candidat', 'honest', 'seri', 'crit', '8211']\n",
      "Preview of document positive_cinerapadura1.html: ['crit', 'pred', 'mai', 'pi', 'caminh']\n",
      "Preview of document positive_cinemasim6.html: ['os', 'jov', 'tit', 'aca', 'no']\n",
      "Preview of document positive_cinemasim2.html: ['marvin', 'crit', '8211', 'cinem', 'sim']\n",
      "Preview of document positive_omelete3.html: ['red', 'joan', 'function', 'var', 'ts']\n",
      "Preview of document positive_cineclick7.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_cinemaemcena3.html: ['os', 'incri', 'cinem', 'cen', 'br']\n",
      "Preview of document positive_elpais2.html: ['nic', 'tod', 'fest', 'futur', 'cult']\n",
      "Preview of document positive_elpais3.html: ['hotel', 'transilvan', 'halloween', 'mar', 'cult']\n",
      "Preview of document positive_pocilga0.html: ['crit', 'rod', 'gigant', 'wond', 'wheel']\n",
      "Preview of document positive_cinerapadura6.html: ['crit', 'mamm', 'mia', 'la', 'vam']\n",
      "Preview of document positive_cinemaemcena8.html: ['dovlatov', 'cinem', 'cen', 'br', 'public']\n",
      "Preview of document positive_cinemasim3.html: ['sierr', 'burges', 'los', 'par', 'alem']\n",
      "Preview of document positive_elpais6.html: ['escob', 'traica', 'mais', 'escob', 'apen']\n",
      "Preview of document positive_elpais5.html: ['039', 'missa', 'impossi', '039', 'espion']\n",
      "Preview of document positive_elpais4.html: ['mamm', 'mia', 'obrig', 'music', 'nao']\n",
      "Preview of document positive_planocritico4.html: ['crit', 'batman', '8211', 'retorn', '8211']\n",
      "Preview of document positive_cineclick9.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_cinemaemcena0.html: ['voc', 'nunc', 'estev', 'real', 'aqu']\n",
      "Preview of document positive_cinerapadura3.html: ['crit', 'barrac', 'beij', 'ai', 'barrac']\n",
      "Preview of document positive_planoaberto6.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_omelete2.html: ['nasc', 'uma', 'estrel', 'function', 'var']\n",
      "Preview of document positive_cinemasim8.html: ['as', 'herd', 'crit', '8211', 'cinem']\n",
      "Preview of document positive_pocilga9.html: ['crit', 'pulp', 'fiction', 'pocilg', 'img.wp']\n",
      "Preview of document positive_pocilga3.html: ['crit', 'rampag', 'destruica', 'total', 'rampag']\n",
      "Preview of document positive_planoaberto4.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_elpais8.html: ['tram', 'fantasm', 'sadomasoqu', 'mod', 'cult']\n",
      "Preview of document positive_cinemaemcena6.html: ['am', 'dupl', 'cinem', 'cen', 'br']\n",
      "Preview of document positive_jornalparaiba3.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_cineclick3.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_jornalparaiba4.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_cineclick8.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_omelete6.html: ['mid', '90', 'function', 'var', 'ts']\n",
      "Preview of document positive_cineclick6.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_cinerapadura5.html: ['crit', 'fre', 'maquin', 'sucess', 'jam']\n",
      "Preview of document positive_planocritico3.html: ['crit', 'misteri', 'relogi', 'pared', '8211']\n",
      "Preview of document positive_cinemasim9.html: ['mamm', 'mia', 'la', 'vam', 'no']\n",
      "Preview of document positive_pocilga12.html: ['crit', 'busc', 'searching', 'pocilg', 'img.wp']\n",
      "Preview of document positive_planoaberto9.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_cinerapadura2.html: ['crit', 'ben', 'agridoc', 'apeg', 'cinem']\n",
      "Preview of document positive_cinemasim0.html: ['gauguin', 'viag', 'tait', 'crit', '8211']\n",
      "Preview of document positive_cinerapadura4.html: ['crit', 'meu', 'ex', 'espia', 'comed']\n",
      "Preview of document positive_jornalparaiba6.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_pocilga2.html: ['crit', 'han', 'sol', 'uma', 'hist']\n",
      "Preview of document positive_planoaberto2.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_planoaberto8.html: ['function', 'h.hj', 'h.hj', 'function', 'h.hj.q']\n",
      "Preview of document positive_jornalparaiba1.html: ['img.wp', 'smiley', 'img.emoj', 'display', 'inlin']\n",
      "Preview of document positive_cineclick0.html: ['function', 'googl', 'analytic', 'object', 'function']\n",
      "Preview of document positive_elpais0.html: ['submersa', 'quim', 'metafis', 'cult', 'el']\n",
      "Preview of document positive_cinemaemcena2.html: ['sicari', 'dia', 'sold', 'cinem', 'cen']\n"
     ]
    }
   ],
   "source": [
    "for key in corpus:\n",
    "    print('Preview of document', key + ':', corpus[key][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação dos dados\n",
    "\n",
    "Primeiro separamos os documentos e seus respectivos ids, para utilizarmos os recursos do `scikit-learn` sem complicações, transformamos nossos documentos (lista de termos) em textos corridos (string única)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate ids from documents\n",
    "ids, documents = zip(*[(id_, ' '.join(document)) for id_, document in corpus.items()])\n",
    "\n",
    "labels = []\n",
    "for id_ in ids:\n",
    "    labels.append(id_.split('_')[0])\n",
    "\n",
    "labels = np.array(labels)\n",
    "ids = np.array(ids)\n",
    "documents = np.array(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No total temos 131 exemplos de documentos não relevantes e 101 de documentos relevantes. Dado que um documento não relevante apresentou erro de leitura, com o intuito de balancear as classes, excluiremos 29 documentos aleatórios dentre os negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of elements: 202\n",
      "Total of negative elements: 101\n",
      "Total of positive elements: 101\n"
     ]
    }
   ],
   "source": [
    "# Loop 29 times\n",
    "for iteration in range(29):\n",
    "    negative_indexes = np.where(labels == 'negative')[0]\n",
    "    index = random.randint(0, len(negative_indexes))\n",
    "\n",
    "    labels = np.delete(labels, index)\n",
    "    ids = np.delete(ids, index)\n",
    "    documents = np.delete(documents, index)\n",
    "\n",
    "print('Total of elements:', len(labels))\n",
    "print('Total of negative elements:', len(np.where(labels == 'negative')[0]))\n",
    "print('Total of positive elements:', len(np.where(labels == 'positive')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Aqui executaremos algumas estratégias para seleção de features. Antes de mais nada, criamos a matriz `term_document`, para tal utilizamos a classe `TfidfVectorizer` do pacote `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<202x101178 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 663778 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 1 and 2-grams features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Documents' order remains the same\n",
    "term_document = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Preview of features: 11 documents, 11133 features\n",
    "term_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui aplicamos a técnica Chi2. Para tal utilizamos como número de features alvo, escolheremos 10% do conjunto inicial de features, equivalente a aproximadamente 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit transform mutual information\n",
    "chi_term_document = SelectKBest(chi2, k=10000).fit_transform(term_document, labels)\n",
    "\n",
    "chi_term_document.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information\n",
    "\n",
    "Novamente, selecionaremos aproximadamente 10% do total de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 10000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit transform mutual information\n",
    "mutual_term_document = SelectKBest(mutual_info_classif, k=10000).fit_transform(term_document, labels)\n",
    "\n",
    "# Shape of transformed term-documents\n",
    "mutual_term_document.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação\n",
    "\n",
    "Aqui são empregados os seguintes algorítmos de classificação:\n",
    "- Naïve\tbayes\n",
    "- Decision tree (J48)\n",
    "- SVM (SMO)\n",
    "- Logistic regression (logistic)\n",
    "- Multilayer perceptron\n",
    "\n",
    "É importante notar que para todos os classificadores, testaremos __três__ _datasets_, um com as originais considerando bigrams, outro com as features transformadas pelo método LSA e por último, um dataset com features transformadas pelo método de mutual information.\n",
    "\n",
    "Antes disso, dividiremos os dados entre treinamento e teste.\n",
    "___\n",
    "\n",
    "## Divisão dos dados\n",
    "\n",
    "Para termos melhor estimativa dos métodos utilizados, aqui definiremos um método para realizar um `K-Fold` nos dados, retornando a média final dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_training(classifier, data, labels, k=5, r=15, verbose=0):\n",
    "    kf = RepeatedKFold(n_splits=k, n_repeats=r)\n",
    "\n",
    "    results = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'fscore': [],\n",
    "        'accuracy': []\n",
    "    }\n",
    "\n",
    "    for train_indexes, test_indexes in kf.split(labels):\n",
    "        train_data, test_data = data[train_indexes], data[test_indexes]\n",
    "        train_labels, test_labels = labels[train_indexes], labels[test_indexes]\n",
    "\n",
    "        # Train and test\n",
    "        classifier.fit(train_data, train_labels)\n",
    "        pred_labels = classifier.predict(test_data)\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(\n",
    "            test_labels, pred_labels, average='weighted'\n",
    "        )\n",
    "        accuracy = accuracy_score(test_labels, pred_labels)\n",
    "\n",
    "        # If verbose, print fold results\n",
    "        if verbose > 1:\n",
    "            print(\n",
    "                classification_report(test_labels, pred_labels)\n",
    "            )\n",
    "            print('Accuracy:', accuracy)\n",
    "\n",
    "        # Save metrics\n",
    "        results['precision'] = precision\n",
    "        results['recall'] = recall\n",
    "        results['fscore'] = fscore\n",
    "        results['accuracy'] = accuracy\n",
    "    \n",
    "    # If verbose, print final results\n",
    "    if verbose > 0:\n",
    "        print(\n",
    "            'Precision mean:', results['precision'].mean(),\n",
    "            'Recall mean:', results['recall'].mean(),\n",
    "            'Fscore mean:', results['fscore'].mean(),\n",
    "            'Accuracy mean:', results['accuracy'].mean(),\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Aqui utilizamos o `MultinomialNB` pois ele aceita uma matriz esparça como input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7,\n",
       " 'recall': 0.6,\n",
       " 'fscore': 0.5878787878787879,\n",
       " 'accuracy': 0.6}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can train with sparse matrix\n",
    "nb = MultinomialNB()\n",
    "\n",
    "kfold_training(nb, term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7646666666666666,\n",
       " 'recall': 0.725,\n",
       " 'fscore': 0.7213702074167191,\n",
       " 'accuracy': 0.725}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can train with sparse matrix\n",
    "nb = MultinomialNB()\n",
    "\n",
    "kfold_training(nb, chi_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6549999999999999,\n",
       " 'recall': 0.65,\n",
       " 'fscore': 0.6508771929824562,\n",
       " 'accuracy': 0.65}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can train with sparse matrix\n",
    "nb = MultinomialNB()\n",
    "\n",
    "kfold_training(nb, mutual_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'fscore': 1.0, 'accuracy': 1.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "kfold_training(dt, term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.906060606060606,\n",
       " 'recall': 0.9,\n",
       " 'fscore': 0.9007672634271099,\n",
       " 'accuracy': 0.9}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "kfold_training(dt, chi_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'fscore': 1.0, 'accuracy': 1.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "kfold_training(dt, mutual_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6994301994301995,\n",
       " 'recall': 0.675,\n",
       " 'fscore': 0.6647324306898775,\n",
       " 'accuracy': 0.675}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(C=0.5, kernel='rbf', degree=8, gamma=0.01, probability=True)\n",
    "\n",
    "kfold_training(svclassifier, term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.22562500000000002,\n",
       " 'recall': 0.475,\n",
       " 'fscore': 0.3059322033898305,\n",
       " 'accuracy': 0.475}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(C=0.5, kernel='rbf', degree=8, gamma=0.01, probability=True)\n",
    "\n",
    "kfold_training(svclassifier, chi_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.20249999999999999,\n",
       " 'recall': 0.45,\n",
       " 'fscore': 0.2793103448275862,\n",
       " 'accuracy': 0.45}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(C=0.5, kernel='rbf', degree=8, gamma=0.01, probability=True)\n",
    "\n",
    "kfold_training(svclassifier, mutual_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6470959595959596,\n",
       " 'recall': 0.625,\n",
       " 'fscore': 0.6261726078799249,\n",
       " 'accuracy': 0.625}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solvers: 'liblinear', 'sag', 'saga'\n",
    "lg = LogisticRegression(random_state=0, solver='sag', multi_class='ovr')\n",
    "\n",
    "kfold_training(lg, term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8293103448275861,\n",
       " 'recall': 0.725,\n",
       " 'fscore': 0.7113475177304964,\n",
       " 'accuracy': 0.725}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solvers: 'liblinear', 'sag', 'saga'\n",
    "lg = LogisticRegression(random_state=0, solver='sag', multi_class='ovr')\n",
    "\n",
    "kfold_training(lg, chi_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.593162393162393,\n",
       " 'recall': 0.525,\n",
       " 'fscore': 0.5151477058453803,\n",
       " 'accuracy': 0.525}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solvers: 'liblinear', 'sag', 'saga'\n",
    "lg = LogisticRegression(random_state=0, solver='sag', multi_class='ovr')\n",
    "\n",
    "kfold_training(lg, mutual_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8125, 'recall': 0.75, 'fscore': 0.75, 'accuracy': 0.75}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "\n",
    "kfold_training(mlp, term_document, labels, k=5, r=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8, 'recall': 0.8, 'fscore': 0.8, 'accuracy': 0.8}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "\n",
    "kfold_training(mlp, chi_term_document, labels, k=5, r=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/leoschet/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8156010230179028,\n",
       " 'recall': 0.775,\n",
       " 'fscore': 0.783367139959432,\n",
       " 'accuracy': 0.775}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "\n",
    "kfold_training(mlp, mutual_term_document, labels, k=5, r=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise\n",
    "\n",
    "Dado os bons resultados do `DecisionTree`, testaremos o `RandomForest`, um ensemble de árvores de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8541353383458647,\n",
       " 'recall': 0.825,\n",
       " 'fscore': 0.8266499057196732,\n",
       " 'accuracy': 0.825}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=200)\n",
    "\n",
    "kfold_training(rf, term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.9055137844611529,\n",
       " 'recall': 0.9,\n",
       " 'fscore': 0.9005050505050505,\n",
       " 'accuracy': 0.9}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=200)\n",
    "\n",
    "kfold_training(rf, chi_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.85, 'recall': 0.85, 'fscore': 0.85, 'accuracy': 0.85}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=200)\n",
    "\n",
    "kfold_training(rf, mutual_term_document, labels, k=5, r=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale a pena notar, que diferentes execuções resultarão em diferentes resultados, mesmo que o `random_state` seja declarado. Isso provavelmente ocorre pelas instancias escolhidas no k-fold. Para suprir tal característica, repetimos o 5-fold 30 vezes. A escolha de repetir 30 vezes vem do fato de que, na estatística, a partir de 30 exemplos, os dados seguem a distribuição normal (ou próximo dela). Mesmo utilizando este artifício, os resultados variaram.\n",
    "\n",
    "A termo de informação, o melhor resultado encontrado foi de 97% para precisão, recall, fscore e acurácia.\n",
    "\n",
    "Testes foram executados, e a melhor combinação de parâmetros foi quando utilizamos 100 estimadores com profundidade máxima de 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistindo o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decisiontree.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist model\n",
    "joblib.dump(dt, 'decisiontree.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['randomforest.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create best classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=200)\n",
    "\n",
    "# Fit using all data available\n",
    "rf.fit(mutual_term_document, labels)\n",
    "\n",
    "# Persist model\n",
    "joblib.dump(rf, 'randomforest.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para carregar o modelo em memória use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist model\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['featureselector.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature selector\n",
    "selector = SelectKBest(mutual_info_classif, k=10000)\n",
    "\n",
    "# Fit using mutual information\n",
    "selector.fit(term_document, labels)\n",
    "\n",
    "# Persist model\n",
    "joblib.dump(selector, 'featureselector.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = joblib.load('decisiontree.joblib')\n",
    "vectorizer = joblib.load('vectorizer.joblib')\n",
    "selector = joblib.load('featureselector.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 10616 and input n_features is 96004 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-dd9f431a08d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_document\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    362\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/inforetrieval/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    385\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 10616 and input n_features is 96004 "
     ]
    }
   ],
   "source": [
    "new_document = 'aqui esta um documento novo nunca visto pela base ele foi previamente tokenizado e em seguida transformado numa string unica'\n",
    "\n",
    "vec = vectorizer.transform([new_document])\n",
    "new_vec = selector.transform(vec)\n",
    "rf.predict(vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
